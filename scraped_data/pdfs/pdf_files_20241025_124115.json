{
    "total_articles": 5,
    "articles": [
        {
            "url": "https://moritzlaw.osu.edu/sites/default/files/2023-12/RobertsLewisBlog3.pdf",
            "file_path": "scraped_data\\pdfs\\8e21686e99f6764356e70e6c8b333233.pdf",
            "text": "Clearview AI: A Google Search for Faces \n \nNicola Roberts-Lewis \n \nIn 2016, an Australian tech-entrepreneur set out to create “A Google search for faces.”1 \nSince then, the program, Clearview AI (Clearview) has transformed into a software used by law \nenforcement and state governments throughout the country to identify people.2 Armed with a \ncache of 40 billion photographs scrubbed from the internet, Clearview created a program that can \ntake a photo of an unidentified person at various locales, such as at an ATM robbery or a political \nprotest, process it through the algorithm, and obtain output of other photos of the individual from \nother locations, such as Venmo accounts or a university’s website.3  \n \nBrief History of Clearview and the Technology \n \nWith just a general idea of how to proceed, Clearview’s founders Hoan Ton-That and \nRichard Schwartz set out by hiring engineers to develop a program to “scrape” websites of \npictures of people’s faces.4 Generally, web scraping uses script to extract data from websites.5 \nClearview specifically mined a variety of websites, such as employment sites, news sites, \neducational sites, and social networks (including Facebook, YouTube, Twitter, Instagram and \neven Venmo).6  The results of this scraping allowed Clearview to amass a database of over 40 \nbillion Facial images.7 After the images were collected, a facial-recognition algorithm was fine-\ntuned to convert all the faces in the scraped images into mathematical vectors based on facial \ngeometry.8 When a user, such as law enforcement, uploads a photo to Clearview for \nidentification , the program maps the person’s image and compares its vectors to those stored in \nClearview’s database.9 \n \nWho is using Clearview? \n \nClearview was initially marketed as a tool for law enforcement agencies, and in 2019, \nState police agencies experienced remarkable success with the program.10 Clearview hired \nRepublican officials as representatives of Clearview. These representatives approached law \nenforcement agencies around the country offering free trials and annual licenses for program \nuse.11 Since then, US agencies, such as Homeland Security and the FBI, have used the program \n \n1 Kashmir Hill, The Secretive Company That Might End Privacy as We Know It, NY TIMES (Nov. 2, 2021), \nhttps://www.nytimes.com/2020/01/18/technology/clearview-privacy-facial-recognition.html; Facial Recognition Firm Faces \nPossible £17m Privacy Fine, BBC (Nov. 29, 2021), https://www.bbc.com/news/business-59466803.  \n2 Hill, supra note 1.  \n3 Id. \n4 Id.  \n5 Tsaone Swaabow Thapelo, SASSCAL WebSAPI: A Web Scraping Application Programming Interface to Support Access to \nSASSCAL’s Weather Data, 20 DATA SCIENCE JOURNAL 24, 3.  \n6 Hill, supra note 1.  \n7 Company Overview, CLEARVIEW AI, https://www.clearview.ai/overview (last visited Dec. 3, 2023). \n8 Hill, supra note 1. \n9 Id.  \n10 Hill, supra note 1 (within 20 minutes of using Clearview, Indiana State Police were able to solve a crime based on security \nfootage). \n11 Id.  \nto track individuals captured on footage from the January 6 Capital Insurrection.12 Additionally, \nthe program has been used by the Ukrainian military to identify both Russian spies trying to \ninfiltrate Ukraine’s population and Russian casualties of the war. 13 The program was also used to \ndetermine if anti-fascists trespassed an event at which Donald Trump attended.14 \n \nSecurity and Privacy Concerns \n \nWhile the program is being marketed as a tool that stops crime, very real threats and \nissues accompany the program. First, what happens when this technology falls into the wrong \nhands? It is being licensed to police departments and state police forces throughout the country \nwith no obvious safeguards on the departments use of the application. For example, a man (with \na Clearview license) could take a photo of a woman in a bar and just with that information \npossibly learn where she lives and works.15 Additionally, Clearview could be used by \nauthoritarian governments to track political enemies. The program can also be used to determine \nthe identify of a woman leaving an abortion clinic with the purposes of doxing her. \nClearview also suffers from problems that plague other facial-recognition AI programs. \nClearview’s facial recognition is not perfect and people have been wrongfully accused of \ncrimes.16 A well-known and supportable criticism of facial recognition technology is that it is \nracially biased.17 Such biases can occur as a result of program-developer bias or because of  \nracially skewed datasets.18 Clearview is not immune to these biases.  \n \nRecent Litigation  \n \nIn 2020, the American Civil Liberties Union filed suit against Clearview, claiming that \nthe company violated the Illinois Biometric Information Privacy Act (BIPA).19 BIPA is a \ngroundbreaking law that was passed in 2008 in Illinois. Biometric data includes retina or iris \nscans, fingerprints, hand scans, facial geometry, DNA, and other unique biological information.20 \nIn the case of Clearview and facial recognition, the biometric information in the ACLU’s case \nwas facial geometry. BIPA only allows private companies to collect biometric data if they: \n• \nInform the person in writing of what [biometric] data is being collected or stored. . . . \n• \nInform the person in writing of the specific purpose and length of time the for which the \n[biometric] data will be collected, stored and used. . . .  \n \n12 Fresh Air, Exposing the Secretive Company At The Forefront Of Facial Recognition Technology, NPR (Sep. 28, 2023), \nhttps://www.npr.org/2023/09/28/1202310781/exposing-the-secretive-company-at-the-forefront-of-facial-recognition-technology.  \n13 Id.  \n14 Id.  \n15 Kashmir Hill, Your Face Belongs to Us: A Secretive Startup’s Quest to End Privacy as We Know It (2023).  \n16 Fresh Air, supra note 12.  \n17 Alex Najibi, Racial Discrimination in Face Recognition Technology, SCIENCE IN THE NEWS (Oct. 24, 2020),  \nhttps://sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/.  \n18 Nada Hassanin, Law Professor Explores Racial Bias Implications In Facial Recognition Technology, UNIVERSITY OF CALGARY \n(Aug. 23, 2023), https://ucalgary.ca/news/law-professor-explores-racial-bias-implications-facial-recognition-technology.  \n19 Settlement Agreement and Release between plaintiff American Civil Liberties Union and Defendant Clearview AI, (May 4, \n2022), https://www.aclu.org/sites/default/files/field_document/exhibit_2_signed_settlement_agreement.pdf.  \n20 Biometric Information Privacy Act (BIPA), ACLU ILLINOIS, https://www.aclu-il.org/en/campaigns/biometric-information-\nprivacy-act-bipa (last visited Dec. 3, 2023).  \n• \nObtain the person’s written consent.21  \nThe lawsuit concluded in May 2022 when the ACLU and Clearview reached a settlement \nagreement.22 The settlement included a ban on Clearview from licensing the program to (1) any \nprivate entity or private individuals (with some exceptions in line with Illinois state law) and (2) \nany government employee not acting in their official capacity. In addition to these licensing bans, \nClearview also agreed to remove faces that Clearview had provided to private individuals from \nits system and for Illinois residents to opt-out of being a part of the program (as in accordance \nwith BIPA).23 \n \nClearview Is Not the Only One \n \nClearview is not the company one would expect to be introducing such a powerful and \nfar-reaching program of identification. Instead, we might think that Google or Meta would have \ndeveloped this program. In fact, according to a past Google chairman, Google had this capability \nin 2011, but determined not to bring the product to any market, even law enforcement24 because \nwas too dangerous.25  \nWhile large tech companies retained this technology but did not use it, other smaller \ncompanies than Clearview offer facial recognition services to private entities. For example, \nMadison Square Garden implemented facial recognition technology at its large-events venues for \nsecurity purposes. However, almost fulfilling the prophecy of the technology being too \ndangerous, the owner of Madison Square Garden used the technology to keep out his “enemies,” \nincluding attorneys employed by companies suing him.26 Other programs include PimEyes. \nSimilar to other platforms that sanction multiple users, its use is not limited to government \nagencies. I could upload a picture of my face right now and maybe get images of myself \ndownloaded from PimEyes.27 As technology advances, more and more programs like this may \nbecome available, and they may be made available to private individuals.  \n \n \n \n21 Id. \n22 Settlement Agreement, supra note 19.  \n23 Id.  \n24 Fresh Air, supra note 12.  \n25 Id.  \n26 Id.  \n27 Id.",
            "metadata": {
                "source_domain": "moritzlaw.osu.edu",
                "scrape_date": "2024-10-25T12:40:28.579070",
                "content_type": "pdf",
                "extraction_method": "pymupdf"
            }
        },
        {
            "url": "https://int.nyt.com/data/documenthelper/6690-clearview-faq/c8b081a0bcca12e7903a/optimized/full.pdf",
            "file_path": "scraped_data\\pdfs\\5c5684873cc92baf9c61c71812d90f8f.pdf",
            "text": "Clearview FAQ\nClearview Subscription\nQ :What do I getas part of a Clearview subscription ?\nA :Youhaveunlimited accessto Clearview 'sresearch technology and image database\nfrom youriPhone,Android ,or Desktop computer.\nQ : How long is my free trial?\nA :Youhave 30 days ofunlimiteduseofClearviewtechnology Afterthat,if you are interested\nin purchasing Clearview , please put us in contact with your supervisororpurchasing\nmanagerto begin the procurementprocess. Wewillnot automatically shutoffyouraccount\nafter 30 days,andweare happy to talk to you if you needmore time.\nQ :When doesmytrial officially begin ?\nA: Your 30 day trialbeginswhen you login for the firsttime.\nQ : WhatifI signed up for an accountand never heard back from Clearview ?\nA: After you requestaccess,Clearview willsendyou an invite to setup your account.If\nyoudon'tseethisinvite within a few days,check yourjunkmail,as sometimes the invites\nget lost there. Ifyou still don't see it, contactusathelp @ clearview .aiand wewillwork\nwith you to setup your account.\nQ : How can my colleagues sign up for a free trial?\nA : You can refer them to ourhomepage,https://clearview .\n,where they can request\naccess,or you can emailus theirnameand emailathelp\n@ clearview .ai andwewillbe\nhappy to send them an invite.\nSearch Results\nQ :Where do Clearview 's data sources come from ?\nA : Clearview ' s data is all gathered from publicly available sources ,includingnews sites, social\nmedia ,mugshotsand more.\nQ : How exactly does Clearview ' s technology work ?\nA :When you upload a photo to Clearview ,our software analyzes the hundredsof features that\nmake up the face and search for a matching face in our image database .Whether the\nsoftware finds no results , similar results , or possible results is determined by how closely the\nfacialfeatures match thoseofanother face in our database . The\ncertain thatthe uploaded\nface and search results match , the lower the delta number willbe. You can find the delta\nnumberby hovering over the word \"possible\" or\" similar\" in the search results.\nClearview FAQ\nQ : What if I am not getting search results ?\nA : If you are notgetting searchresults,thephoto maybe tooblurry orhave a bad angle.If\npossible, try a high resolution photo with a clearfrontalview ofthe face.We are working on\nimproving ourimage -sharpening technology, butcurrentlya photo shouldworkevenifthe\nsuspectgrowsabeard,wearsglasses , or appearsinbad lighting.Ifyourdesktop ishaving a\nhardtimepicking up aface in anuploaded image,you can try focusing yourphonecameraon\nthefaceand use theClearviewapp to search. Or, if you areusingyour phonetotake a picture\nof aphoto on yourcomputer and thereflection is interferingwith theresults ,try uploading\nthe photo to the Clearview software on your desktop .\nIf you stillarenotgettingresults,try thephoto again in aweek ortwo.Weare addingmillionsof\nnew photos to ourdatabase every day, so youmayget a match in a few weeks even ifyou\ndon'trightnow .\nQ : How accurate are Clearview ' s search results ?\nA :Clearview hasthemostaccuratefacialidentification software in theworld , with a 98 .6 %\naccuracy rate. This doesnotmean thatyou willgetmatches for 98 .6 %\nyoursearches ,but\nyouwill almostneverget a falsepositive. You will eithergeta correctmatch ornorresults.\nWehave a 30 -60 % hitrate,butweare addinghundredsofmillionsofnewfaceseverymonth\nandexpect to get to 80 %\nby the end of2019.\nQ : Can I upload myown photos/gallery to Clearview ?\nA Yes! Clearview can help you importyour photos from whatever format they're in (JSON,\nCSV, SQL ,\nother) andmanage them foryou. You willbe able to search your ownphotos\nwith the accuracyofClearview 's algorithm . Search resultsfrom yourowngallery will appear\nalongside Clearview search results . You can setthe results to be only accessible to yourown\nusers, or opt-in to share itwith otherlaw enforcementagencies. You can choose to sharethe\nphotoswith them outright or a blurred version where you can vet each request. Clearview will\nnevershare yourinformation with anyoneelsewithoutyouragency' s consent.\nThisfeatureis currently under development, please emailhelp @ clearview .aiifyou want to be\nthe first users to havethis.\nPrivacy & Security\nQ : Who can seemy search history ?\nA : Only the administrator ofyour organization can see your searchhistory . The administrator\nhastherightto addandremoveusersin theirorganization , and they can do audits to make\nsure that the toolis being used properly .\nQ : What is my search history retention ?\nA\nBydefault, searches areretained foreverunless an administrator usesthe secure delete\ntool. Administrators can change the settings to automatically purge search history after 30 days,\nwhich means theoriginal image, database entry, and search results willbe purged from all\nClearview systems, including thebackups.\nClearview FAQ\nQ : Am\nallowedto useClearview results aspositiveID or asevidence in court ?\nA : Search results establishedthrough Clearview Alandits related systemsand\ntechnologiesareindicativeandnotdefinitive. Clearview Al,Incmakesno guarantees as\nto the accuracy ofits search-identification software. Law enforcementprofessionals\nMUST conduct\nresearch in order to verify identities or other data generatedby the\nClearview Alsystem . Clearview Alis neither designed nor intended to be used as a\nsingle-source system for establishingtheidentityofan individual.Furthermore,\nClearview Alisneitherdesigned nor intended to beused as evidence in a courtoflaw .\nQ : Where is Clearview's data hosted?\nA: Clearview Aimaintains andmanagesitsowncloudstorage system in the United States\nofAmerica.Wehavecompletedmultipleindependentsecurity reviews, and have a\ndedicated cybersecurity team whichmonitorsandprotects thesecurity ofuseraccountsand\ndata. Clearview Ai\nhas cybersecurity insurance.\nQ : Are the images\nsearch shared with anyone else?\nA : Probeimagessubmittedto Clearview Aiare notsharedwith anythirdparties. Clearview\ncustomersupportemployeescannotview anyuser' s probeimagesand cannot view the image\nsearch resultswhich users are obtaining.\nAboutthe Company\nQ : How\nis Clearview differentthan other facial identification companies ?\nA: Clearview ' sfacialidentification software ismore accurate than thatofanyother company.\nThe softwareisable to findamatchingfaceoutofabillion faces, rather than simply comparing\ntwofaces.Wealso havea largerdatabase than other facial identification companies, with\nalmost 2 billion faces, andwe are adding millionsmore everyday.\nQ : How many agencies use Clearview ?\nA : Over 300 agencies nationwide are atvarious\nof contracting for Clearview 's\nsoftware,including agenciesatthefederal,state, and locallevels.If you would like references or\nsuccess stories, feelfree to contactus and wewould behappy to send you thatinformation.\nQ: Who can I contact at Clearview ?\nA : You can contactourhelp desk at anytimebyemailinghelp @ clearview .ai.\nClearview FAQ\nw",
            "metadata": {
                "source_domain": "int.nyt.com",
                "scrape_date": "2024-10-25T12:40:29.154952",
                "content_type": "pdf",
                "extraction_method": "pymupdf"
            }
        },
        {
            "url": "https://www.privacylaws.com/media/4767/tues-1215-clearview.pdf",
            "file_path": "scraped_data\\pdfs\\29fd4be635606496755dd42fbe7e1286.pdf",
            "text": "©2024 Arnold & Porter Kaye Scholer LLP. All Rights Reserved.\nClearview wins appeal \nagainst £7.5m ICO \nfine, but AI firms are \nnot in the clear\n1\nSession participants\nJeff Brown\nGeneral Officer/Counsel, \nGlobal Privacy\nValentina De Giorgis\nAssociate General \nCounsel – DPO \nJames Castro-Edwards\nCounsel\nTELUS International AI\nTELUS International AI\nArnold & Porter Kaye Scholer (UK) LLP\n2\n• Delaware-incorporated company, no EU or UK establishment;\n• Clients in US, Panama, Brazil, Mexico and Dominican Republic; no EU or UK clients; \n• Only offered services to government agencies or government agency contractors; \n• Exclusively for law enforcement / national security purposes and did not provide services outside this \ncontext; \n• Clearview AI has not offered services to commercial clients since 2020, following a settlement with the \nAmerican Civil Liberties Union.\nBackground – Clearview AI\n3\n• Clearview AI created a database of over 20 billion facial images, collected from the internet, along with the \nURL, link to social media page and name of the profile; \n• Using machine learning, faces were grouped by similarities and assigned specific vectors \n• So similar looking faces were digitally stored closer together;\n• Clearview AI provided an identification service that enabled its clients to identify individuals; \n• Clients upload a ‘probe image’ of an individual to the Clearview AI database, which then provides a list of \nindividuals’ faces that bear a close resemblance;\n• Clearview AI does not say whether or not the images are the same person; this decision is made by the \nclient.\nBackground – Clearview AI (2)\n4\n• The ICO announced a joint investigation with the Office of the Australian Information Commissioner \n(“OAIC”) on 3rd November 2021;\n• The ICO took the view that, given the vast number of images collected, some would be from the UK; \n• In addition, at least 5 UK law enforcement agencies had used the services on a trial basis, which returned \nnumerous matches involving UK residents;\n• Images of individuals’ faces, metadata and URLs constituted ‘personal data’; \n• Facial vectors constituted biometric data, one of the ‘special categories’;\n• Collecting, storing, applying vectors and comparing to ‘probe’ images amounts to ‘processing’.\nICO Investigations\n5\n• Clearview AI’s processing took place both before and after the end of the Brexit implementation period \n(11pm, 31st Dec. 2020), so both the GDPR and UK GDPR applied;\n• Clearview AI came within the scope of Art. 3(2)(b) of both the GDPR and the UK GDPR, since it concerned \n‘monitoring’ the behaviour of individuals in the UK;\n• The ICO noted that the CNIL had taken a similar position regarding jurisdiction / Art. 3(2)(b);\nApplication of the UK GDPR / GDPR\n6\n• Clearview AI’s processing breached the following principles:\n– Fairness, Lawfulness & Transparency\n– Storage Limitation\n– No lawful basis for processing personal data\n– No lawful basis for processing special categories of personal data\n– No transparency information\n– Failure to uphold data subjects’ rights\n– Failure to carry out a data protection impact assessment\n• The ICO issued Clearview AI with a fine of £7,552,800 in May 2022; and\n• An enforcement notice ordering Clearview AI to:\n– Stop collecting UK citizens’ data; and\n– Delete any UK citizens’ data within 6 months. \nBreaches of the UK GDPR / GDPR\n7\n• Clearview AI appealed to the First Tier Tribunal on the basis that it is a foreign company, providing services \nto “foreign clients, using foreign IP addresses, and in support of the public interest activities of foreign \ngovernments and government agencies, in particular in relation to their national security and criminal law \nenforcement functions”.\n• The Tribunal agreed that Clearview AI’s activities fell within the territorial scope; but\n• Clearview AI’s processing fell outside the material scope of the GDPR / UK GDPR, as it concerned the acts \nof foreign governments;\n• The GDPR and UK GDPR provisions are constructed differently:\n– The acts of foreign governments fall outside the material scope of the GDPR; and\n– Do not constitute ‘relevant processing’ for the purposes of Art. 3 UK GDPR\nClearview AI’s appeal\n8\n• The decisive factor was that Clearview AI provided services to government agencies and their contractors \nfor law enforcement and national security purposes; narrow grounds that are unlikely to be available for \nmost commercial businesses;\n• However, the decision affirms the extra-territorial reach of the UK GDPR; and\n• The ICO will take enforcement action against foreign companies engaged in non-compliant “data scraping”;\n• Businesses that develop or use AI models using UK citizens’ personal data, wherever they are located, \nshould be aware of the Clearview case and should follow the ICO’s AI Guidance.\nImplications for companies\n9\nTELUS International AI Data Solutions\n• Creating and enhancing the world's data to enable better AI via human intelligence\n• We help companies test and improve machine learning models via our global AI Community of 1 million+ \nannotators and linguists.\n• Our services include: Data collection and creation, Data annotation, Data validation and relevance, \nlinguistic annotation, Gen AI Services: dataset engineering, content generation, model validation.\nTELUS International AI\n10\n• What our Legal team asks before starting a project: \n– What data are you collecting?\n– What for? \n– Where from? \n– How are you obtaining the data?\n• If the purpose is to harvest personal data from the internet, our general advice is not to do it \n• Data scraping from other sources (books/newspapers): \n– What are you borrowing? \n– The intrinsic value of the author´s production? Or: check punctuation / other uses related to the use of words?\n– Are you making the site unable to operate due to the amount of traffic? Be mindful of automated means: site \ndisruption.\n– Anti-scraping provisions in YouTube and other sites: TCs\nTI AI: AI training – general considerations\n11\n• The data that TI AI obtains comes from the Community of 1 million+ annotators and linguists.\n• The data is targeted, comes from the person directly and we ensure we have the appropriate consent forms \nin place.\n• For non-sensitive data, the legal basis is the performance of a contract with the Community member.\n• Always informed: transparency is key to maintaining the trust of our Community.\nTI AI: Contributors’ data\n12\n• Cars with cameras and lidar sensors collect images of people on the street.\n• The objective is to train the algorithms of self-driving cars. \n• Not for the purpose of publication, unlike Google Streetview. \n• License plates and faces may inadvertently be collected but this collection is incidental.\nTI AI: Automated driving algorithm\n13\n• Collect thumbnails of search outputs on two major client platforms. \n• The objective of this was to see how accurate the voice search function was on both platforms.\n• We provided the Community of people who tested the voice search function in several languages, then \noffered insights on the results.\n• TI AI didn’t collect any videos, only the search results: video names, and the image (Thumbnail) of the \nvideo. \nTI AI – Thumbnail search outputs\n14\n• TELUS International: Next-generation, digitally-led customer experiences.\n• There are large data sets of call recordings in our client systems.\n• Purpose limitation of the processing of personal data.\n• If requested by the client as the data controller, we would be able to provide AI Data Solution services \nrelated to these datasets.\nTELUS International Call recording data\n15\nQuestions?",
            "metadata": {
                "source_domain": "www.privacylaws.com",
                "scrape_date": "2024-10-25T12:40:29.725275",
                "content_type": "pdf",
                "extraction_method": "pymupdf"
            }
        },
        {
            "url": "https://www.dpa.gr/sites/default/files/2022-08/35_2022%20anonym_EN_FINAL.pdf",
            "file_path": "scraped_data\\pdfs\\01605ffaad4c6d92547974015451102f.pdf",
            "text": "1-3 Kifisias Avenue, 11523 Athens, Tel: 210 6475600, Fax: 210 6475628, contact@dpa.gr / www.dpa.gr  \n \n \nAthens, 13-07-2022 \nRef. No: 1809 \n \nDECISION 35/2022 \nThe Hellenic Data Protection Authority convened remotely on 19-04-2022, following the \nmeeting of 29-03-2022, following an invitation of its President, to examine the case \nmentioned in the present case history. The President of the Authority, Konstantinos \nMenoudakos and the regular members of the Authority, Grigorios Tsolias and Christos \nKalloniatis as rapporteurs, Spyridon Vlachopoulos, Konstantinos Lambrinoudakis, \nCharalambos Anthopoulos and Aikaterini Iliadou attended. Present, without voting rights, \nwere Fotini Karvela, Maria Alikakou, Anastasia Kaniklidou, Kyriaki Karakasi, legal auditors — \nlawyers, as well as Georgios Roussopoulos and Pantelis Kammas, IT auditors, as assistant \nrapporteurs and Eirini Papageorgopoulou, employee of the Administration Department, as \nsecretary.  \nThe Authority took into account the following:  \nRef. No. G/IN/3458/26-05-2021 complaint submitted to the Authority by the non-profit civil \norganization named ‘Homo Digitalis’ on behalf of the complainant, A, is in principle a \ncomplaint of a breach of A’s right of access vis-à-vis the United States-based company \nClearview AI (St.214 W 29th St,2nd Floor, New York City, NY, 10001). That complaint, which also \nseeks an examination, on the whole, of the practices of the defendant company from the \npoint of view of the protection of personal data, was submitted simultaneously with four \nother relevant complaints before the supervisory authorities of Austria, France, Italy and the \nUnited Kingdom, aiming to a coordinated response to the practices of that company by the \ncompetent supervisory bodies.  \nIn the context of the present case, the complainant sent an e-mail to the company concerned \non 24 March 2021, exercising her right of access under Article 15 of the General Data \nProtection Regulation (Regulation (EU) 2016/679 — GDPR) to her personal data processed by \n \n2 \nthat company, while on the same date she received confirmation of the successful receipt of \nthat request by the recipient. Afterwards, on 26 April 2021, the complainant reintroduced the \nabove request with a reminder message to the defendant. On 30 April 2021, the complainant \nwas informed by a representative of Clearview AI that the request submitted by email had \nnot been detected and was asked to attach a photograph of her, so that her request is \nforwarded as urgent, if she had used an e-mail address other than the one through which she \nmade the request for the first time. On 5-5-2021 and in response to the above, the \ncomplainant sent an electronic confirmation of receipt of her request from the defendant \ndated 24 March 2021, while on 26 May 2021 she submitted the complaint in question to the \nAuthority.     \nThe Authority, in its examination of the above complaint, by Ref. no. G/IN/4752/16-07-2021 \ndocument addressed the defendant company and, after recalling the provisions of Articles \n3(2) and 27 of the GDPR on the territorial scope of the GDPR and on representatives of \ncontrollers or processors not established in the European Union (hereinafter: EU), asked the \ncompany for information on the details of its representative in the EU, if it is based in a \ncountry outside the EU. In the event that the company has an establishment within the EU, a \nseries of questions were submitted concerning the identity of the controller or processor for \nthe processing in question, the possibility of more than one establishment of the controller \nor processor on the territory of the EU and the indication of the main establishment in the \nevent of more than one such establishment. In addition, and further to the above questions, \nclarification of the nature of the processing as cross-border was requested either in the sense \nthat it is carried out in the context of the activities of any several establishments of the \ndefendant in several Member States, or in the sense that it affects or is likely to substantially \naffect data subjects in several Member States. Finally, the above questions also included \ninformation as to whether the complainant exercised a right as a data subject and, if so, what \nwas the defendant’s response to it and within which deadline.  \nThen, by Ref. No. G/IN/5303/16-08-2021, the defendant company, after claiming that it is not \nsubject to the GDPR, stated that it is based in the U.S. and does not have an establishment in \nthe EU. It then challenged the application of Article 3(2) of the GDPR, since it allegedly does \nnot provide products or services to data subjects within the EU, nor does it monitor the \nbehavior of data subjects within the EU. The defendant stressed that its services are provided \n \n3 \nto government law enforcement authorities outside the EU, while denying that the creation \nby its own search engine of links to photos available online constitutes monitoring of the \nbehavior of data subjects, as these are instant image projections without any \nsystematic/continuing observation of each person. According to the defendant, there is no \nGDPR implementation scope, as it has a search engine that automatically displays results on \nthe basis of the most relevant algorithm in relation to the question introduced by a third \nparty. In fact, the defendant concluded that there was a breach of public international law in \nthe event that a company providing online services is obliged to comply with all laws \nworldwide. Then, and in the context of bona fide and voluntary assistance, as noted by the \ncomplainant, in relation to the present case, it confirmed that the request for access was \nmade on 24 March 2021 by the complainant, who also submitted a reminder message on 26-\n4-2021. It then referred to the existence of a technical problem which prevented its \nrepresentative from reading the file submitted by the complainant with her photograph in \norder to respond to her request, and although the company’s standard practice in such cases \nis to request a photograph from the subject again, in the present case, it inadvertently replied \nto the complainant by sending the standard e-mail for cases where the request itself is not \ndetected. Finally, the defendant stated that it had complied with the request for access by \nallegedly sending its reply to the complainant. However, it should be noted that by a \nsupplementary letter from the complainant with ref. no. G/IN/4976/22-03-2022 it appears \nthat she had received no reply from the defendant. \nAll the information in the file shows the following for the company in question: \nClearview AI, Inc. is based in the United States and was founded in 2017. Its unique product \nis a facial recognition platform, which allows users to associate photos of faces present in the \ncompany’s database with photos of them on the internet. Its platform, according to what is  \nstated on its website1 “is supported by facial recognition technology and includes the largest \nknown database containing more than ten (10) billion facial images from public online \nsources, including news, websites, signage photos, public social media and other public \nsources”.  \n                                                           \n1 https://www.clearview.ai/overview (retrieved 26/5/2022 — translation from English) \n \n4 \nThis complaint states that according to publicly available sources2 and the conclusions \nreached by other EU supervisory authorities, which have examined similar complaints against \nClearview AI, Inc.,3 the facial recognition tool provided by the defendant operates as follows: \n1. The company collects, through the use of ‘web scraping’ techniques, images \ncontaining human faces from social networks (particularly Facebook and Twitter), \nblogs and generally websites on which publicly accessible photos are available, as well \nas videos available online (e.g. YouTube). Together with these images, the company \nalso collects information it extracts from these photos, including geolocation \nmetadata that the photo may contain and information derived from the appearance \nof the person’s face in the photos4. This information is stored in Clearview’s database.  \n2. The company processes the images using special techniques, so that every person \nshown in a photograph is converted into a certain numerical sequence, which is called \n“vector” and is recognizable by machines.  \n3. The above numerical sequences are stored in the company’s database and are \nfragmented for the listing of the database and for future identification of persons. \nThus, each person in the database has a separate vector and a fragmented value \nassociated with it.  \n4. When a user of Clearview services wants to identify a person, they post an image and \nperform a search. Clearview analyses this image and extracts a vector for the face on \nthe image, which then fragments and compares against all fragmented vectors stored \nin its database. Finally, the company extracts each identified image from its database \nand provides a list of results, containing all the corresponding images and metadata. \nIf a user clicks on any of these results, he/she is directed to the source page of the \nimage. \n                                                           \n2 See Joint Investigation of Clearview by the Data Protection Supervisory Authorities of Canada, Quebec, British \nColumbia and Alberta, https://www.priv.gc.ca/en/opc-actions-and-decisions/investigations/investigations-\ninto-businesses/2021/pipeda-2021-001/#toc7. See also the U.S. Patent and Trade Marks Register: \nhttps://tmsearch.uspto.gov/bin/showfield?f=doc&state=4803:tnmzul.2.1  \n3 See decision of the Supervisory Authority for the Protection of Personal Data of Hamburg \nhttps://noyb.eu/sites/default/files/2021-01/545_2020_Anh%C3%B6rung_CVAI_ENG_Redacted.PDF. Decision of the \nGarante https://edpb.europa.eu/news/national-news/2022/facial-recognition-italian-sa-fines-clearview-ai-eur-\n20-million_en. Decision of CNIL https://www.cnil.fr/en/facial-recognition-cnil-orders-clearview-ai-stop-reusing-\nphotographs-available-internet.  \n4 See (at the date of the meeting) Clearview AI Privacy Policy, Inc. https://www.clearview.ai/privacy-policy \n \n5 \n \nFinally, the Authority, by Ref. no. G/OUT/887/11-4-2022 document, called the defendant \ncompany, sending at the same time a translation into English of the complaint at issue, so \nthat it can be heard in the videoconference of 19-4-2022 of the Plenary of the Authority. \nHowever, the defendant did not appear and subsequently the Authority proceeded to the \nexamination of the file and, after hearing the rapporteurs and the clarifications from the \nassistant rapporteurs, and after a thorough discussion, \n \nDELIBERATED ACCORDING TO THE LAW \n \n1. In accordance with Article 3(2)(b)GDPR, \"this Regulation applies to the processing of \npersonal data of data subjects located in the Union by a controller or processor not established \nin the Union, if the processing activities relate to: b) monitoring their behavior to the extent \nthat such behavior takes place within the Union.” \nIn that regard, recital 24 of the GDPR provides, in relation to the inclusion of a processing \nwithin the territorial scope of the GDPR on the basis of Article 3(2)(b), that “... in order to \ndetermine whether a processing activity can be considered to monitor the conduct of a data \nsubject, it should be ascertained whether individuals are tracked online, including the potential \nsubsequent use of personal data processing techniques consisting of shaping a natural \nperson’s ‘profile’, in particular with a view to making decisions concerning him or her or \nanalyzing or predicting his or her personal preferences, behaviors and attitudes.” \nThe EDPB Guidelines 3/2018 on the territorial scope of the GDPR clarify in this regard that \n“contrary to the provision of Article 3(2)(a), neither Article 3(2)(b) nor recital 24 explicitly \nestablishes a required degree of “targeting intention” on the part of the controller or processor \nin order to determine whether the monitoring activity could trigger the application of the \nGDPR to the processing activities. However, using the word “monitoring” implies that the \ncontroller has a specific intention in mind to collect and subsequently re-use the relevant data \non a person’s behavior within the EU. The EDPB does not consider that any online collection \nor analysis of personal data in the EU is automatically regarded as ‘monitoring’. It is necessary \nto examine the purpose of the controller for the processing of the data and, in particular, for \n \n6 \nany subsequent use of behavioral analysis or profiling techniques that include such data. The \nEDPB takes into account the wording of recital 24, which states that in determining whether \nthe processing can be considered to monitor the behavior of a data subject, a key parameter \nis to monitor individuals online, including the potential subsequent use of profiling \ntechniques.” \n2. Article 4 par. 1 GDPR defines personal data as any information relating to an identified or \nidentifiable natural person (‘data subject’); an identifiable natural person is one who can be \nidentified, directly or indirectly, in particular by reference to an identifier such as a name, an \nidentification number, location data, an online identifier or to one or more factors specific to \nthe physical, physiological, genetic, mental, economic, cultural or social identity of that \nnatural person.  \n3. Furthermore, in accordance with Article 4 par. 4 profiling means “any form of automated \nprocessing of personal data consisting of the use of personal data to evaluate certain personal \naspects of a natural person, in particular to analyses or predict aspects relating to the \nperformance at work, economic situation, health, personal preferences, interests, reliability, \nbehavior, location or movements of that natural person”.  \nIn this regard, the Guidelines on automated decision-making and profiling for the purposes of \nArticle 29 Working Party Regulation 2016/679 specify that profiling must: (a) be an automated \nform of processing, (b) relate to personal data, and (c) assess personal aspects of a natural \nperson, while stressing that “the widespread availability of personal data on the internet and \nfrom Internet of Things (IoT) devices, as well as the ability to find associations and establish \nlinks, may enable the identification, analysis and prediction of aspects of a natural person’s \npersonality or behavior and interests and habits”5.  \n4. According to Article 4(14) GDPR, biometric data means “personal data resulting from \nspecific technical processing linked to the physical, physiological or behavioral characteristics \nof a natural person and which allow or confirm the unequivocal identification of that natural \nperson, such as facial images or dactyloscopic data”.  \n                                                           \n5 Guidelines on automated decision-making and profiling for the purposes of Article 29 Working Party Regulation \n2016/679, WP251Rev.01, 3 October 2017 as finally revised and adopted on 6 February 2018, p. 5, 7, \nhttps://ec.europa.eu/newsroom/article29/items/612053 \n \n \n7 \nIn addition, according to Article 9(1) GDPR, the special categories of personal data requiring \nspecial protection include biometric data for the purpose of the unambiguous identification \nof a person.  \nIn that regard, recital 51 of the GDPR states that photographs of persons are covered by the \ndefinition of biometric data only in the case of processing by means of specific technical \nmeans allowing unambiguous identification or verification of a natural person’s identity.  \n5. According to Article 55(1) GDPR, “each supervisory authority shall be competent to carry \nout the tasks and exercise the powers conferred on it in accordance with this Regulation in the \nterritory of its Member State”. \nArticle 56(1) GDPR stipulates that “without prejudice to Article 55, the supervisory authority \nof the main or single establishment of the controller or processor shall be competent to act as \nlead supervisory authority for the cross-border processing operations of that controller or \nprocessor in accordance with the procedure laid down in Article 60”. \nThe GDPR cooperation mechanism (art. 60 et seq. GDPR) applies only in the case of a \ncontroller or processor with one or more establishments in the EU6. Similarly, recital 122 of \nthe GDPR states that “Each supervisory authority should be competent, in the territory of its \nMember State, to exercise the powers and perform the tasks conferred on it in accordance \nwith this Regulation. This should cover in particular processing (...) carried out by a controller \nor processor not established in the Union, when targeting data subjects residing in its \nterritory”.  \nTherefore, in the case of a controller without an establishment in the EU, which falls within \nthe scope of the GDPR on the basis of the targeting criterion set out in Article 3(2) GDPR, each \nnational supervisory authority is competent to verify its compliance with the GDPR on the \nterritory of its Member State.  \n6. Where Article 3(2) GDPR applies, Article 27 provides that the controller is obliged to \ndesignate in writing a representative in the EU, who must be established in one of the \nMember States where the data subjects are located, whose data are processed in connection \n                                                           \n6 See Guidelines of Art. 29 WP for the identification of the lead authority of a controller or processor, WP 244, \nhttps://ec.europa.eu/newsroom/article29/document.cfm?doc_id=44102 \n \n8 \nwith an offer of goods or services to them or whose conduct is monitored, unless one of the \nexceptions provided for in paragraph 2 of that Article 27 applies.  \nThe representative, as explained in recital 80, must act on behalf of the controller, and any \nsupervisory authority may be addressed to him. The representative shall be appointed by \nwritten instruction of the controller to act on its behalf in respect of its obligations under the \nGDPR. The EDPB Guidelines 3/2018 on the territorial scope of the GDPR stipulate that this \nmainly entails obligations relating to the exercise of the rights of data subjects, and in this \ncontext the provision of the representative’s identity and contact details to data subjects in \naccordance with the provisions of Articles 13 and 14 GDPR. Although he/she is not responsible \nfor complying with the rights of data subjects, the representative should facilitate \ncommunication between the subjects and the represented controller in order to ensure the \neffective exercise of the rights of the subjects. As explained in recital 80, the representative \nshould also be subject to enforcement procedures in case of non-compliance by the \ncontroller. This means, in practice, that it must be ensured that a supervisory authority is able \nto communicate with the representative on any matter relating to the compliance obligations \nof a controller established outside the EU and that the representative must be able to \nfacilitate any exchange of information or procedures between the requesting supervisory \nauthority and the controller or processor established outside the EU. \n7. Article 5(1) of the GDPR lays down the principles that should govern a processing operation. \nIn particular, paragraph 1 provides that: “1. Personal data are: (a) processed lawfully and fairly \nin a transparent manner in relation to the data subject (“lawfulness, objectivity and \ntransparency”), [...] (e) kept in a form that allows the identification of the data subjects only \nfor the period necessary for the purposes of the processing of personal data (“limitation of the \nstorage period”). In order to ensure that the data are kept no longer than necessary, recital \n39 clarifies that the controller should set deadlines for their deletion or for their periodic \nreview.  \nIn accordance with the principle of accountability introduced by the second paragraph of that \nArticle, it is expressly stated that the controller “shall be responsible and able to demonstrate \ncompliance with paragraph 1 (“accountability”). This principle, which is a cornerstone of the \nGDPR, entails the obligation for the controller to be able to demonstrate compliance, \n \n9 \nincluding the legal documentation of any processing operation carried out in accordance with \nthe legal bases provided by the GDPR and national data protection law. \nAny processing of personal data shall be lawful only if at least one of the conditions set out in \nArticle 6(1) GDPR applies, such as: “(a) the data subject has consented to the processing of \nhis or her personal data for one or more specific purposes; (b) the processing is necessary for \nthe performance of a contract to which the data subject is party or to take measures at the \nrequest of the data subject prior to entering into a contract; (c) processing is necessary to \ncomply with a legal obligation of the controller; (d) the processing is necessary to safeguard \nthe vital interest of the data subject or other natural person; (...),(f) processing is necessary for \nthe purposes of the legitimate interests pursued by the controller or by a third party, unless \nthose interests override the interests or fundamental rights and freedoms of the data subject \nwhich require the protection of personal data, in particular if the data subject is a child.” If one \nof the above conditions is met, it is also the legal basis for processing.  \nIn that regard, with the principle of transparency, recital 39 of the GDPR provides, inter alia, \nthat “it should be clear to individuals that personal data concerning them are collected, used, \ntaken into account or otherwise processed, and to what extent personal data are or will be \nprocessed. That principle requires that any information and communication relating to the \nprocessing of such personal data be easily accessible and understandable and use clear and \nsimple language”. As also stated in recital 60, which refers to the rights of information, which \nimplement, inter alia, the principle of transparency7, “the principles of fair and transparent \nprocessing require that the data subject be informed of the existence of the processing \noperation and its purposes”.  \n8. Furthermore, as stated in recital 51 of the GDPR, special categories of data require special \nprotection, since the context of their processing could create significant risks to fundamental \nrights and freedoms. While, therefore, in order for the processing of “simple” personal data \nto be lawful, it is sufficient to have one of the legal bases of Article 6, the processing of special \ncategories of data is, in principle, prohibited and allowed only if one of the legal bases of \nArticle 6 and one of the exceptions of Article 9(2) GDPR apply. This view is endorsed both by \nOpinion 6/2014 of Article 29 WP on the concept of legitimate interests of the controller under \n                                                           \n7 See also the Transparency Guidelines under Regulation 2016/679 of Art. 29 WP, WP 260, p. 7.  \n \n10 \nArticle 7 of Directive8 95/46 and by the EDPB in its Guidelines 8/2020 on targeting social \nmedia users9. It is therefore not lawful to process special categories of data if Article 6 is not \nadhered to and only the exceptions referred to in Article 9 are met. In this spirit, Article 29 \nWP Opinion 6/2014 on the concept of legitimate interests of the controller in Article 7 of \nDirective 95/46 states that \"it would be wrong to conclude that the fact that someone has \nmanifestly disclosed specific categories of data pursuant to Article 8(2)(e) of Directive 95/46 \n(now Article 9(2)(e) GDPR) would be — always in itself — a sufficient condition to allow any \nkind of data processing without an assessment of the balancing of the interests and rights at \nstake, as required by Article 7(f) of Directive 95/46 (now Article 6(1)(f) of the GDPR)10.  \n9. The right to information enshrined in Articles 13 and 14 GDPR provides, where the data \nhave not been collected from the data subject (art. 14(3)(a) that the controller shall provide \nthe information referred to in paragraphs 1 and 2 (identity of controller, purposes and legal \nbasis for processing, categories of data, etc.) “within a reasonable period of time from the \ncollection of personal data, but no later than one month, taking into account the specific \ncircumstances in which the personal data are processed”. In particular, as clarified by the \nTransparency Guidelines under Regulation 2016/679 of the Art. 29 WP, “the data subject \nshould be able to determine in advance the scope of the processing and the consequences it \nentails, and should not be surprised at a later stage as to the ways in which his or her personal \ndata have been used”. \n10. According to Article 15(1), (3) and (4) of the GDPR “1. The data subject shall have the right \nto obtain from the controller confirmation as to whether or not personal data concerning him \nor her are being processed and, if so, the right of access to personal data to the following \ninformation”. \nArticle 12(2), (3) and (4) GDPR provides that \"The controller shall facilitate the exercise of the \nrights of data subjects provided for in Articles 15 to 22. [...] 3. The controller shall provide the \ndata subject with information on the action taken on request pursuant to Articles 15 to 22 \n                                                           \n8 See Opinion 6/2014 on the concept of legitimate interests of the controller within the meaning of Article 7 of \nDirective 95/46, p. 14. \n9 See Guidelines 8/2020 on targeting social media users, p. 40.  \n10 See CJEU, C-13/16, Valsts policijas Rīgas reрiona pārvaldes Kārtības policijas pārvalde v Rīgas pašvaldības SIA \n‘Rīgas satiksme’, 4 May 2017, concerning the cumulative conditions to be satisfied in order for data processing \nto be lawful on the basis of ‘legitimate interests’. \n \n \n11 \nwithout delay and in any event within one month of receipt of the request. That period may \nbe extended by a further two months, if necessary, taking into account the complexity of the \nrequest and the number of requests. The controller shall inform the data subject of such \nextension within one month of receipt of the request and of the reasons for the delay. [...] 4. \nIf the controller does not act on the data subject’s request, it shall inform the data subject \nwithin one month of receipt of the request of the reasons why it did not act and of the \npossibility of lodging a complaint with a supervisory authority and seeking a judicial remedy”.  \n 11. In the present case, as it is apparent from the defendant’s privacy policy (available at \nhttps://www.clearview.ai/privacy-policy), the latter collects “Information derived from \npublicly available photos: As part of Clearview’s normal business operations, it collects photos \nthat are publicly available on the Internet. Clearview may extract information from those \nphotos including such geolocation metadata as the photo may contain and information \nderived from the facial appearance of individuals in the photos.  \nThose photographs are indisputably personal data as defined in Article 4(1) GDPR, in so far as \nthey allow the identification of a natural person by reference to one or more factors specific \nto his or her physical or physiological identity. The same conclusion has been reached by the \nCJEU, which has held that “the image of a person recorded by a camera constitutes personal \ndata within the meaning of that provision (Article 2(a) of Directive 95/46) in so far as it enables \nthe person concerned to be identified.”11 \nFurthermore, it follows from the defendant’s privacy policy that it collects photographs that \nare publicly available on the internet without applying any geographical selection criterion. \nThis wideness of the collection is an inherent feature of the service which the defendant sells. \nIt is worth noting that in previous versions of its privacy policy, it explicitly12 stated that it \ncollects data from subjects located in the EU.  \n12. As regards the link between the defendant’s processing activities and the monitoring of \nthe behavior of subjects in the EU, it is crucial whether these subjects are monitored online.  \nThe processing, carried out by the defendant, results in the production of a search outcome \n— on the basis of a photograph posted by the user of its services — which contains all the \n                                                           \n11 Case C-212/2013, František Ryneš v Úřad pro ochranu osobních údajē. \n12 In force on 29.1.2020.  \n \n12 \nphotographs that have a shared fragmented vector with the photo posted by the user. In this \nway, a profile is created about a person, which consists of the photographs in which that \nperson appears, as well as their metadata, i.e. the URLs of the websites where those photos \nare located. The association of these photographs and the context in which they are \npresented on a website allows the collection of much information about the person, his/her \nhabits and preferences. In particular, when a photo is posted on social networks or on a \nwebsite that publishes articles, or on a blog, this may result in the collection of information \nthat allows the person’s behavior to be determined. The analysis of the above information \nthat a person chooses to make public on the internet and the context in which he or she \nchooses to make it public, ultimately enables that person’s online behavior to be determined \non the basis of his or her own personal or professional life exposure options.  \nConsequently, the automated processing of personal data described above for the purpose \nof assessing the personal aspects of a natural person constitutes profiling and the making \navailable to users of the defendant’s services, who search the defendant’s facial recognition \nplatform, constitutes surveillance on the internet. Moreover, the purpose of the tool \nmarketed by the defendant is to enable the identification and collection of information in \nrelation to a particular person. Biometric processing techniques used by the defendant to \nenable a person to be targeted ultimately lead to profiling as a result of a search by a user of \nthe defendant’s tool. This search is renewed over time, as the database is constantly updated, \nwhich makes it possible to establish the possible evolution of information relating to a \nparticular person, in particular if the results of successive searches are compared with each \nother.  \n13. Clearview AI, Inc., is based in the United States and has no establishment in the EU. The \nGDPR cooperation mechanism (art. 60 et seq. GDPR) applies only in the case of a controller \nor processor with one or more establishments in the EU13. Similarly, recital 122 of the GDPR \nstates that “Each supervisory authority should be competent, in the territory of its Member \nState, to exercise the powers and perform the tasks conferred on it in accordance with this \nRegulation. This should cover in particular processing (....) carried out by a controller or \nprocessor not established in the Union, when targeting data subjects residing in its territory”.  \n                                                           \n13 See Guidelines of Art. 29 WP for the identification of the lead authority of a controller or processor, WP 244, \nhttps://ec.europa.eu/newsroom/article29/document.cfm?doc_id=44102 \n \n13 \nTherefore, in the case of Clearview AI, Inc., which does not have an establishment in the EU \nbut falls within the scope of the GDPR on the basis of the targeting criterion set out in Article \n3(2) GDPR, each national supervisory authority is competent to verify compliance with the \nGDPR on the territory of its Member State, as stated above.  \n14. Furthermore, in the present case, since Clearview AI, Inc. falls within the scope of the \nGDPR, under Article 3(2)(b) without having an establishment in the EU, it is obliged to appoint \na representative in the EU in accordance with Article 27 GDPR, but has not fulfilled it.  \n15. In the present case, the data subjects whose data are processed by the defendant do not \nreceive any information from the defendant, through that privacy policy, in relation to any of \nthe elements referred to in Article 14 GDPR, either before or even after the processing. In \nfact, data subjects may never learn that their data has been processed by the defendant \nunless they randomly read a publication about Clearview AI, Inc. \n16. The principle of the lawfulness of processing has the meaning that, in order to be lawful, \nthe processing to which the data are subject must be based on one of the legal bases provided \nfor in Article 6 GDPR.  \nIn the present case, none of the documents in the file reveals the existence of any of the legal \nbases provided for in Article 6.  \nIn particular, it is not established —neither would it be possible on the basis of the \ncharacteristics of the processing in question— that there is consent of the subjects (6 (1)(a) \nGDPR), or performance of a contract between the data subject and the controller (6(1)(b) \nGDPR), or compliance with a legal obligation of the controller (6(1)(c) GDPR), or the \nsafeguarding of a vital interest of the subject (6(1)(d) GDPR).  \nAs regards, in particular, the possible application of the legal basis referred to in paragraph \n1(f) of this Article, it is provided that processing is lawful where it is necessary for the purposes \nof the legitimate interests pursued by the controller or a third party, provided that the \ninterests or fundamental rights and freedoms of the data subject which require the protection \nof personal data do not prevail over those interests. In relation to that provision, recital 47 of \nthe GDPR states that, after balancing the legitimate interests of the controller or third party \nand the interests or fundamental rights of the subject, the legitimate interests of the former \nmust not prevail over the interests or rights of the data subject, taking into account the \n \n14 \nlegitimate expectations of the data subject on the basis of his or her relationship with the \ncontroller.  \nThe legitimate expectations of the data subject in relation to the processing of his or her data \nare highlighted as a factor taken into account in the above balancing also by the WP of Article \n29 in Opinion 6/2014 on the concept of the legitimate interests of the controller under Article \n7 of Directive 95/46. The same Opinion also clarifies that personal data are still considered \npersonal data and subject to the necessary protection requirements, even if they have \nbecome public. That said, the fact that personal data is publicly available can be considered a \ncritical factor when assessing legitimate interests, especially if the publication was made with \na reasonable expectation of re-use of the data for specific purposes (e.g. for research \npurposes or for purposes related to transparency and accountability). \nIn the present case, given that there is no relationship between the subjects and the \ndefendant, nor can there be any reasonable expectation of the subjects that their online \nphotographs will be processed by a facial recognition platform, the existence of which they \nare likely to be unaware of, the conditions for the application of Article 6(1)(f) GDPR are not \nfulfilled.  \nFrom all the information brought to the attention of the Authority, it emerged that the \nprocessing in question does not concern a simple collection of data, but results in the \nconversion of the photographs collected into biometric data, the processing of which is \nsubject to the strictest provisions of Article 9 GDPR. In that regard, bearing in mind that the \nprocessing of special categories of data is in principle prohibited and permitted only if one of \nthe legal bases referred to in Article 6 and one of the exceptions set out in Article 9(2) GDPR \ncumulatively apply and that none of the legal bases of Article 6 GDPR exist in relation to the \nprocessing of critical data, it appears that the processing of biometric data carried out by the \ndefendant does not meet the legal requirements laid down by the GDPR.  \n17. From the information brought to the attention of the Authority, it emerged that, although \nthe complainant exercised the right of access to her personal data under Article 15 GDPR, via \ne-mail to the defendant on 24 March 2021, with which the defendant agrees (ref. no. \nG/IN/5303/16.08.2021 its reply to the Authority), however, she has never received any reply \nand her right of access has never been satisfied by the defendant, in accordance with \ndocument with ref. no. G/IN/4976/22.03.2022 from the complainant to the Authority and \n \n15 \ncontrary to what the defendant claims in its above letter to the Authority, in which it states \nthat it sent the complainant a standardized message.  \n18. In the light of the above, from the information in the file, the Authority finds on behalf of \nthe complainant company, Clearview AI, Inc.: \nA) Breach of the obligation to designate a representative in the EU (Article 27 GDPR) because, \nalthough the defendant falls within the scope of the GDPR, under Article 3(2)(b) without \nhaving an establishment in the EU and is therefore obliged to appoint a representative in the \nEU in accordance with Article 27 GDPR, it has not fulfilled its obligation.  \nB) Breach of the principle of the lawfulness of processing (Article 5(1)(a), 6 and 9 GDPR), \nbecause the processing carried out by the defendant is not based on any legal basis from the \nprovisions of Article 6 GDPR, while none of the exceptions of Article 9 GDPR apply with regard \nto the special categories of data.  \nC) Breach of the principle of transparency of processing (Article 5(1)(a) GDPR) and of the \nrelated right of information of the subjects (Article 14 GDPR), because the defendant did not, \nas required, inform the subjects whose data is processed accurately and clearly about the \ncollection and use of their personal data.  \nD) Breach of the complainant’s right of access (Articles 12 and 15 GDPR), because the \ndefendant did not comply with the request made by the complainant, as set out above.  \n19. In the light of the above, the Authority considers that it is appropriate to exercise its \ncorrective powers under Articles 58(2)(i) and 83 GDPR (imposition of a fine) in respect of all \nthe above infringements, its corrective powers under Article 58(2)(c) of the GDPR with regard \nto the satisfaction of the complainant’s right of access and its corrective powers under Article \n58(2)(g) and (f) with regard to the erasure of personal data of subjects located in the Greek \nterritory and the prohibition of their processing. In determining the fine, which the Authority \nconsiders to be effective, proportionate and dissuasive, account shall be taken of the \nmeasurement criteria set out in Article 83(2) GDPR applicable in the present case, as \ninterpreted in particular by the Guidelines on the application and setting of administrative \nfines for the purposes of Regulation 2016/679 of the Article 29 Working Party. \nEspecially, particular account shall be taken of: \n \n16 \nA) the nature, gravity and duration of the infringement, which is not an isolated incident, but \nis systematic and concerns the basic principles of the lawfulness of the processing (art. 5, 6, 9 \nGDPR), which are fundamental to the protection of personal data, in accordance with the \nGDPR. It should be pointed out that compliance with the principles laid down in Article 5 of \nthe GDPR is of paramount importance, and above all, the principle of lawfulness, so that, if it \ndoes not exist, the processing would be unlawful from the outset, even if the other principles \nof processing have been adhered, especially in the present case where none of the legal bases \nfor the processing referred to in Articles 6 and 9 of the GDPR has not been established, as set \nout above. \nB) the number of affected subjects in the Greek territory, which due to the data collection \ntechniques used by the defendant is potentially very high. Indeed, it does not follow from the \nprivacy policy of the defendant and the way in which personal data is collected that a relevant \ntechnique is applied which excludes some of the photographs of individuals with specific \ncriteria. \nC) The fact that the processing at issue concerns special categories of personal data \n(biometric).  \nD) The degree of responsibility of the defendant, which is high, taking into account that the \nprocessing in question continues despite the intervention of supervisory authorities inside \nand outside the EU.  \nE) The failure of the defendant to cooperate with the Authority, taking into account that the \ndefendant did not attend the meeting of the Authority even though it was invited. \nF) The fact that, in accordance with the provisions of Article 83(5)(a) and (b) of the GDPR, \ninfringement of the basic principles for processing and the rights of the subjects falls within \nthe upper category of the system of administrative fines. \nG) The fact that data on the defendant’s turnover are not available to the Authority.  \n \nOn the basis of the aforementioned, the Authority unanimously decides that the \nadministrative penalty referred to in the operative part of the decision, which is deemed, as \nmentioned above, proportionate to the gravity of the infringement, must be inflicted on the \ndefendant, as controller. \n \n17 \n \nFOR THESE REASONS \nThe Authority \nA. Imposes on the defendant company Clearview AI, Inc., based in the USA, 214 W 29th St, 2nd \nFloor, New York City, NY, 10001, as controller, on the basis of Article 58(2)(i) GDPR, a total \nfine of EUR 20 million (EUR 20 000 000) for breaching the principles of lawfulness and \ntransparency (art. 5 paras 1a, 6, 9 GDPR) and its obligations under Articles 12, 14, 15 and 27 \nof the GDPR. \nB. Οrders the defendant US-based company Clearview AI, Inc., 214 W 29th St, 2nd Floor, New \nYork City, NY, 10001, as the controller, on the basis of Article 58(2)(c) GDPR to comply with \nthe complainant’s request for the exercise of her right of access. \nC. Ιmposes on the defendant company under the name Clearview AI, Inc., based in the USA, \n214 W 29th St, 2nd Floor, New York City, NY, 10001, as the controller, on the basis of Article \n58(2)(f) of the GDPR, the prohibition of the collection and processing of personal data of \npersons located in the Greek territory, using the methods included in the facial recognition \nservice which it trades.  \nD. Orders the defendant company under the name Clearview AI, Inc., based in the USA, 214 \nW 29th St, 2nd Floor, New York City, NY, 10001, as the controller, on the basis of Article 58(2)(g) \nof the GDPR, to erase the personal data of subjects located in the Greek territory, which it \ncollects and processes using the methods included in the facial recognition service which it \ntrades.  \n \n \nThe President \n \n \nThe Secretary  \nKonstantinos Menoudakos \nEirini Papageorgopoulou",
            "metadata": {
                "source_domain": "www.dpa.gr",
                "scrape_date": "2024-10-25T12:40:29.768090",
                "content_type": "pdf",
                "extraction_method": "pymupdf"
            }
        },
        {
            "url": "https://www.ourcommons.ca/Content/Committee/441/ETHI/Reports/RP11948475/ethirp06/ethirp06-e.pdf",
            "file_path": "scraped_data\\pdfs\\3574adc4eb6b6c633e6b5e79eacddcd3.pdf",
            "text": "FACIAL RECOGNITION TECHNOLOGY \nAND THE GROWING POWER OF \nARTIFICIAL INTELLIGENCE\nReport of the Standing Committee on Access to \nInformation, Privacy and Ethics\nPat Kelly, Chair\nOCTOBER 2022\n44th PARLIAMENT, 1st SESSION\nPublished under the authority of the Speaker of the House of Commons \nSPEAKER’S PERMISSION \nThe proceedings of the House of Commons and its Committees are hereby made available to provide greater public access. The \nparliamentary privilege of the House of Commons to control the publication and broadcast of the proceedings of the House of \nCommons and its Committees is nonetheless reserved. All copyrights therein are also reserved. \nReproduction of the proceedings of the House of Commons and its Committees, in whole or in part and in any medium, is \nhereby permitted provided that the reproduction is accurate and is not presented as official. This permission does not extend \nto reproduction, distribution or use for commercial purpose of financial gain. Reproduction or use outside this permission or \nwithout authorization may be treated as copyright infringement in accordance with the Copyright Act. Authorization may be \nobtained on written application to the Office of the Speaker of the House of Commons. \nReproduction in accordance with this permission does not constitute publication under the authority of the House of \nCommons. The absolute privilege that applies to the proceedings of the House of Commons does not extend to these permitted \nreproductions. Where a reproduction includes briefs to a Standing Committee of the House of Commons, authorization for \nreproduction may be required from the authors in accordance with the Copyright Act. \nNothing in this permission abrogates or derogates from the privileges, powers, immunities and rights of the House of Commons \nand its Committees. For greater certainty, this permission does not affect the prohibition against impeaching or questioning the \nproceedings of the House of Commons in courts or otherwise. The House of Commons retains the right and privilege to find \nusers in contempt of Parliament if a reproduction or use is not in accordance with this permission. \nAlso available on the House of Commons website \nat the following address: www.ourcommons.ca \nFACIAL RECOGNITION TECHNOLOGY AND \nTHE GROWING POWER OF ARTIFICIAL \nINTELLIGENCE \nReport of the Standing Committee on \nAccess to Information, Privacy and Ethics \nPat Kelly \nChair \nOCTOBER 2022 \n44th PARLIAMENT, 1st SESSION\n \nNOTICE TO READER \nReports from committees presented to the House of Commons \nPresenting a report to the House is the way a committee makes public its findings and recommendations \non a particular topic. Substantive reports on a subject-matter study usually contain a synopsis of the \ntestimony heard, the recommendations made by the committee, as well as the reasons for those \nrecommendations. \nTo assist the reader: \nA list of abbreviations used in this report is available on page ix \niii \nSTANDING COMMITTEE ON ACCESS TO \nINFORMATION, PRIVACY AND ETHICS \nCHAIR \nPat Kelly \nVICE-CHAIRS \nIqra Khalid \nRené Villemure \nMEMBERS \nParm Bains \nJames Bezan \nHon. Greg Fergus \nMatthew Green \nLisa Hepfner \nDamien C. Kurek \nYa’ara Saks \nRyan Williams \nOTHER MEMBERS OF PARLIAMENT WHO PARTICIPATED \nRichard Bragdon \nIqwinder Gaheer \nJean-Denis Garon \nLeah Gazan \nMajid Jowhari \nArielle Kayabaga \nJennifer O’Connell \nBrad Redekopp \nFrancesco Sorbara \nJoanne Thompson \nAnita Vandenbeld \nDominique Vien \niv \nCathay Wagantall \nCLERK OF THE COMMITTEE \nNancy Vohl \nLIBRARY OF PARLIAMENT \nParliamentary Information, Education and Research Services \nSabrina Charland, Analyst \nAlexandra Savoie, Analyst \nv \nTHE STANDING COMMITTEE ON  \nACCESS TO INFORMATION, PRIVACY AND ETHICS  \nhas the honour to present its \nSIXTH REPORT \nPursuant to its mandate under Standing Order 108(3)(h), the committee has studied the use and \nimpact of facial recognition technology and has agreed to report the following:\n \n \n \nvii \nTABLE OF CONTENTS \nLIST OF ACRONYMS.............................................................................................................................................IX \nSUMMARY ................................................................................................................................................................... 1 \nLIST OF RECOMMENDATIONS....................................................................................................................... 3 \nFACIAL RECOGNITION TECHNOLOGY AND THE GROWING POWER OF \nARTIFICIAL INTELLIGENCE ............................................................................................................................ 7 \nIntroduction....................................................................................................................................................... 7 \nBackground ................................................................................................................................................ 7 \nStructure of the Report ....................................................................................................................... 8 \nChapter 1: Facial Recognition Technology ..................................................................................... 8 \nFacial Recognition Technology: What It Is and How It Works ................................... 8 \nFacial Recognition Technology in 2022................................................................................. 10 \nBenefits of Facial Recognition Technology.......................................................................... 11 \nConcerns About Facial Recognition Technology.............................................................. 13 \nMisidentification and Algorithmic Bias ......................................................................... 13 \nOther Concerns ............................................................................................................................. 15 \nChapter 2: Uses and Related Risks.................................................................................................... 17 \nUse of Facial Recognition by Police Forces.......................................................................... 17 \nCriticism ............................................................................................................................................ 17 \nRisk of Mass Surveillance by Police Forces ................................................................. 19 \nUse by the Royal Canadian Mounted Police ................................................................ 20 \nUse by the Toronto Police Service .................................................................................... 25 \nUse of Facial Recognition by Other Federal Agencies................................................... 27 \nUse of Facial Recognition by Border Authorities............................................................. 28 \nUse of Facial Recognition in Public Spaces .......................................................................... 32 \nUse of Facial Recognition in the Workplace........................................................................ 34 \nviii \nUse of Facial Recognition by Political Parties .................................................................... 35 \nCommittee Observations and Recommendations ........................................................... 35 \nChapter 3: Accountability, Procurement and Public Investment .................................. 36 \nAccountability........................................................................................................................................ 36 \nTransparency ................................................................................................................................. 36 \nGovernance and Accountability.......................................................................................... 38 \nProcurement and Public-Private Partnerships................................................................. 40 \nProcurement by Police Forces............................................................................................. 42 \nPublic Investment ............................................................................................................................... 42 \nExample of Accountability in Action: Microsoft ............................................................... 43 \nCommittee Observations and Recommendations ........................................................... 44 \nChapter 4: Regulating Facial Recognition Technology and Artificial \nIntelligence ...................................................................................................................................................... 46 \nMoratoriums, Bans and Other Measures .............................................................................. 46 \nPrivacy Guidance on Facial Recognition for Police Agencies ................................... 49 \nLegislation................................................................................................................................................ 51 \nLegislative Framework for the Public and Private Sector.................................. 52 \nLegislative Framework for Police Services ................................................................. 57 \nBest Practices in Other Jurisdictions ............................................................................... 58 \nCommittee Observations and Recommendations ........................................................... 62 \nConclusion........................................................................................................................................................ 64 \nAPPENDIX A LIST OF WITNESSES ............................................................................................................ 65 \nAPPENDIX B LIST OF BRIEFS....................................................................................................................... 69 \nREQUEST FOR GOVERNMENT RESPONSE .......................................................................................... 71 \n \nix \nLIST OF ACRONYMS \nACLU \nAmerican Civil Liberties Union \nAI \nArtificial Intelligence \nAIA \nAlgorithmic Impact Assessment \nBIPA \nBiometric Information Privacy Act \nCAI \nCommission d’accès à l’information du Québec \nCBSA \nCanada Border Services Agency \nCCLA \nCanadian Civil Liberties Association \nCIPPIC \nSamuelson-Glushko Canadian Internet Policy and Public Interest Clinic \nCSIS \nCanadian Security Intelligence Service \nFR \nFacial Recognition \nFRT \nFacial Recognition Technology \nICLMG \nInternational Civil Liberties Monitoring Group \nNCCM \nNational Council of Canadian Muslims \nNCECC \nNational Child Exploitation Crime Centre \nNIST \nNational Institute of Standards and Technology \nNTOP \nNational Technologies Onboarding Program \nOECD \nOrganisation for Economic Co-operation and Development \nOPC \nOffice of the Privacy Commissioner of Canada \nPIPEDA \nPersonal Information Protection and Electronic Documents Act \nRCMP \nRoyal Canadian Mounted Police \nx \nTPS \nToronto Police Service \nTPSB \nToronto Police Services Board \n \nSUMMARY \nThe rise of artificial intelligence (AI) and the growing use of facial recognition \ntechnology (FRT), as well as recent investigations by the Office of the Privacy \nCommissioner (OPC) into FRT, led the Committee to study FRT and the growing power \nof AI. \nThis report looks at the benefits and risks of FRT and its use in specific contexts, such \nas law enforcement. It explores other AI governance issues, such as procurement and \npublic investment in this area. It also looks at legislative and other solutions to reassure \nCanadians that the use of FRT or other AI tools in Canada is done responsibly and \nrespects their rights. \nTaking into account witness testimony, the Committee makes several recommendations \nto improve the federal legislative framework that applies to FRT and AI technologies, \nincluding the recommendation to impose a moratorium on the use of FRT in Canada, as \nrecommended by a majority of witnesses.\n \n \n \n3 \nLIST OF RECOMMENDATIONS \nAs a result of their deliberations committees may make recommendations which they \ninclude in their reports for the consideration of the House of Commons or the Government. \nRecommendations related to this study are listed below. \nRecommendation 1 \nThat the Government of Canada amend section 4 of the Privacy Act to require \na government institution to ensure that the practices of any third party from \nwhich it obtains personal information are lawful. ......................................................... 35 \nRecommendation 2 \nThat the Government of Canada ensure that airports and industries publicly \ndisclose the use of facial recognition technology including with, but not limited \nto, signage prominently displayed in the observation area and on the \ntravel.gc.ca website. ........................................................................................................ 35 \nRecommendation 3 \nThat the Government of Canada refer the use of facial recognition technology \nin military or intelligence operations, or when other uses of facial recognition \ntechnology by the state have national security implications, to the National \nSecurity and Intelligence Committee of Parliamentarians for study, review and \nrecommendation; and that the Committee report its findings. .................................... 35 \nRecommendation 4 \nThat the government, in the creation of its regulatory framework around the \nuse of facial recognition technology, set out clear penalties for violations \nby police............................................................................................................................ 36 \nRecommendation 5 \nThat the Government of Canada amend its procurement policies to require \ngovernment institutions that acquire facial recognition technology or other \nalgorithmic tools, including free trials, to make that acquisition public, subject \nto national security concerns........................................................................................... 45 \n4 \nRecommendation 6 \nThat the Government of Canada create a public AI registry in which all \nalgorithmic tools used by any entity operating in Canada are listed, subject to \nnational security concerns. .............................................................................................. 45 \nRecommendation 7 \nThat the Government of Canada enhance the Treasury Board Directive on \nAutomated Decision-Making to ensure the participation of civil society groups \nin algorithmic impact assessments and to impose more specific requirements \nfor the ongoing monitoring of artificial intelligence systems. ....................................... 45 \nRecommendation 8 \nThat the Government of Canada increase its investment in initiatives to study \nthe impact of artificial intelligence on various demographic groups, increase \ndigital literacy, and educate Canadians about their privacy rights................................ 45 \nRecommendation 9 \nThat the Government of Canada ensure the full and transparent disclosure of \nracial, age or other unconscious biases that may exist in facial recognition \ntechnology used by the government, as soon as the bias is found in the context \nof testing scenarios or live applications of the technology, subject to national \nsecurity concerns.............................................................................................................. 45 \nRecommendation 10 \nThat the Government of Canada establish robust policy measures within the \npublic sector for the use of facial recognition technology which could include \nimmediate and advance public notice and public comment, consultation with \nmarginalized groups and independent oversight mechanisms...................................... 45 \nRecommendation 11 \nThat the government define in appropriate legislation acceptable uses of facial \nrecognition technology or other algorithmic technologies and prohibit other \nuses, including mass surveillance. ................................................................................... 62 \n5 \nRecommendation 12 \nThat the Government of Canada amend the Privacy Act to require that prior to \nthe adoption, creation, or use of facial recognition technology, government \nagencies seek the advice and recommendations of the Privacy Commissioner, \nand file impact assessments with his or her office. ........................................................ 63 \nRecommendation 13 \nThat the Government of Canada update the Canadian Human Rights Act to \nensure that it applies to discrimination caused by the use of facial recognition \ntechnology and other artificial intelligence technologies. ............................................. 63 \nRecommendation 14 \nThat the Government of Canada implement the right to erasure (“right to be \nforgotten”) by requiring service providers, social media platforms and other \nonline entities operating in Canada to delete all users’ personal information \nafter a set period following users’ termination of use, including but not limited \nto uploaded photographs, payment information, address and contact \ninformation, posts and survey entries. ........................................................................... 63 \nRecommendation 15 \nThat the Government of Canada implement an opt-in-only requirement for the \ncollection of biometric information by private sector entities and prohibit such \nentities from making the provision of goods or services contingent on providing \nbiometric information. ..................................................................................................... 63 \nRecommendation 16 \nThat the Government of Canada strengthen the ability of the Privacy \nCommissioner to levy meaningful penalties on government institutions and \nprivate entities whose use of facial recognition technology violates the Privacy \nAct or the Personal Information Protection and Electronic Documents Act to \ndeter future abuse of the technology. ............................................................................ 63 \n6 \nRecommendation 17 \nThat the Government of Canada amend the Privacy Act and the Personal \nInformation Protection and Electronic Documents Act to prohibit the practice of \ncapturing images of Canadians from the internet or public spaces for the \npurpose of populating facial recognition technology databases or artificial \nintelligence algorithms..................................................................................................... 63 \nRecommendation 18 \nThat the Government of Canada impose a federal moratorium on the use of \nfacial recognition technology by (Federal) policing services and Canadian \nindustries unless implemented in confirmed consultation with the Office of the \nPrivacy Commissioner or through judicial authorization; that the Government \nactively develop a regulatory framework concerning uses, prohibitions, \noversight and privacy of facial recognition technology; and that the oversight \nshould include proactive engagement measures, program level authorization \nor advance notification before use, and powers to audit and make orders. ................ 64 \nRecommendation 19 \nThat the federal government ensure that appropriate privacy protections are \nput in place to mitigate risks to individuals, including measures addressing \naccuracy, retention and transparency in facial recognition initiatives as well as \na comprehensive strategy around informed consent by Canadians for the use \nof their private information. ............................................................................................ 64 \n \n \n7 \nFACIAL RECOGNITION TECHNOLOGY \nAND THE GROWING POWER OF \nARTIFICIAL INTELLIGENCE \nINTRODUCTION \nBackground \nArtificial intelligence (AI) is omnipresent in society now. Facial recognition technology \n(FRT), which relies on AI, is also becoming more popular. In Canada, FRT was recently the \nsubject of a joint investigation by the Office of the Privacy Commissioner of Canada \n(OPC) and its provincial counterparts in Alberta, British Columbia and Quebec in a case \ninvolving Clearview AI. \nIn February 2021, the OPC released the report of its joint investigation in which it \nconcluded that Clearview AI had failed to comply with the Personal Information \nProtection and Electronic Documents Act (PIPEDA) by engaging in the mass collection of \nimages without consent and for inappropriate purposes.1 \nThe OPC also conducted an investigation into the use of Clearview AI technology by the \nRoyal Canadian Mounted Police (RCMP). In its special report to Parliament, released in \nJune 2021, the OPC concluded that the RCMP had failed to comply with the Privacy Act \nby collecting personal information from a third party (Clearview AI) who illegally \ncollected it.2 \nIn light of the above, in December 2021 the Committee unanimously adopted a motion \nto study the use and impacts of FRT and the growing power of AI. \n \n1 \nOffice of the Privacy Commissioner of Canada (OPC), Joint investigation of Clearview AI, Inc. by the Office of \nthe Privacy Commissioner of Canada, the Commission d’accès à l’information du Québec, the Information \nand Privacy Commissioner for British Columbia, and the Information Privacy Commissioner of Alberta, \n2 February 2021 [OPC Report on Clearview AI]. The OPC concluded that Clearview AI was not exempt from \nobtaining consent under the PIPEDA’s “publicly available” personal information exemption, which is limited \nto publicly available personal information identified in the Regulations Specifying Publicly Available \nInformation. \n2 \nOPC, Police use of Facial Recognition Technology in Canada and the way forward, Special report to \nParliament on the OPC’s investigation into the RCMP’s use of Clearview AI and draft joint guidance for law \nenforcement agencies considering the use of facial recognition technology, 10 June 2021 [Special Report on \nthe RCMP]. The RCMP challenged the OPC’s findings (see Chapter 2 of this report). \n \n8 \nThe Committee held nine public meetings and heard 33 witnesses. It also received eight \nbriefs. The Committee thanks all those who participated in the study. \nStructure of the Report \nThe report is divided into four chapters. Chapter 1 explains what FRT is and its presence \nin the market and provides an overview of its benefits and risks. Chapter 2 focuses on \nspecific uses of FRT, including by law enforcement. Chapter 3 discusses stakeholder \naccountability with respect to using and developing FRT and AI and issues related to AI \nprocurement and public investment. Finally, Chapter 4 focuses on the regulation of FRT \nand AI. \nCHAPTER 1: FACIAL RECOGNITION TECHNOLOGY \n“Like all technologies, FRT can, if used responsibly, offer \nsignificant benefits to society. However, it can also be \nextremely intrusive, enable widespread surveillance, \nprovide biased results and erode human rights, including \nthe right to participate freely, without surveillance, \nin democratic life.” \nDaniel Therrien, Privacy Commissioner of Canada,  \nwho appeared before the Committee on 2 May 2022. \nFacial Recognition Technology: What It Is and How It Works \nCarole Piovesan, a managing partner at INQ Law, said that FRT uses highly sensitive \nbiometric facial data to identify and verify an individual. Brenda McPhail, director of the \nprivacy, technology and surveillance program at the Canadian Civil Liberties Association \n(CCLA), said FRT can be thought of as facial fingerprinting. \nFacial recognition (FR) is the process of identifying a face from a digital image or video. \nFRT can be deployed in real time or on static images. It uses computer pattern \nrecognition to find commonalities in images depicting human faces. FRT can be used to \nconfirm the identity of a known person, or to identify an unknown person. It can also \nallow for the categorization and profiling of a person over time based on their facial \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n9 \ninformation.3 In other words, FRT can be used for verification, identification and \ncategorization/characterization purposes.4 \nIn general, however, FRT systems fall under two categories: those used to verify a \nperson’s identity (one-to-one) and those used to identify an individual (one-to-many). \nA one-to-one system compares a user’s image to multiple images of a \nsingle person to authenticate or verify a person’s known identity. A one-\nto-many system compares an image to a database of different faces (such \nas a terrorist watchlist or mugshot database) to uniquely identify an \nindividual among a group of people, often in live or real-time settings.5 \nElizabeth Anne Watkins, a postdoctoral research associate at Princeton University, \ndescribed facial verification as follows: \nWhereas facial recognition is a 1:n system, which means it both finds and identifies \nindividuals from camera feeds typically viewing large numbers of faces, usually without \nthe knowledge of those individuals, facial verification, on the other hand, while built on \nsimilar recognition technology, is distinct in how it's used. Facial verification is a 1:1 \nmatching system, much more intimate and up close where a person's face, directly in \nfront of the camera, is matched to the face already associated with the device or digital \naccount they're logging in to. If the system can see your face and predict that it's a \nmatch to the face already associated with the device or account, then you're permitted \nto log in. If this match cannot be verified, then you'll remain locked out. If you use Face \nID on an iPhone, for example, you've already used facial verification. \nAngelina Wang, a graduate researcher in computer science at Princeton University, \nexplained that, from a technical standpoint, FRT is a machine-learning model. Rather \nthan applying hand-coded rules, the model is given a very large dataset of faces with \nannotations, from which it learns.6 These annotations include labels noting which \nimages are the same person, and the location of the face in each image. \nMs. Wang said data for these models is typically collected through crowdsourcing on \nplatforms like Amazon Mechanical Turk, which she said is known for having \n \n3 \nCentre for Media, Technology and Democracy and Cybersecure Policy Exchange, Brief to the ETHI \nCommittee – Study on the Use and Impact of Facial Recognition Technology, 1 June 2022, p. 2 [CMTD and \nCPE Brief]. \n4 \nChristelle Tessono, Brief to the ETHI Committee – Study on the Use and Impact of Facial Recognition \nTechnology, 4 May 2022, p. 2 [Tessono Brief]. \n5 \nCMTD and CPE Brief, p. 2. \n6 \nAngelina Wang gave the example of a hand-coded rule saying that two people are more likely to be the \nsame if they have the same coloured eyes. \n \n10 \nhomogeneous worker populations and unfavourable working conditions, or simply \nscraped off the Internet, from websites like Flickr.7 These datasets vary in size, from \n10,000 images up to millions of images. \nFacial Recognition Technology in 2022 \nMany witnesses said that FRT is increasingly present in the market and in society. \nFor example, Ms. Piovesan said that FRT is becoming much more extensively used by \npublic and private sectors alike. She said that, according to a 2020 study published by \nGrand Review Research, the global market size of FRT is expected to reach US$12 billion \nby 2028.8 She said this expansion is due to considerable investments and advancements \nin the use of FRT around the world. She added that, while discussions about FRT tend to \nfocus on security and surveillance, various other sectors are using this technology, \nincluding retail and e-commerce, telecommunications and information technology, and \nhealth care. This presents a growing economic opportunity for developers and users. \nNestor Maslej, a research associate at the Institute for Human-Centered Artificial \nIntelligence at Stanford University, shared the following statistics from the Institute’s \n2022 AI Index Report.9 \nIn 2021, 18 of 24 U.S. government agencies used these technologies: 16 departments \nfor digital access or cybersecurity, six for creating leads in criminal investigations, and \nfive for physical security. Moreover, 10 departments noted that they hoped to broaden \nits use. These figures are admittedly U.S.-centric, but they paint a picture of how widely \ngovernments use these tools and towards what end. \nSince 2017, there has also been a total of $7.5 billion U.S. invested globally in funding \nstart-ups dedicated to facial recognition. However, only $1.6 million of that investment \nhas gone towards Canadian FRT start-ups. In the same time period, the amount invested \nin FRT technologies has increased 105%, which suggests that business interest in FRT is \n \n7 \nFor example, Nestor Maslej, a research associate at the Institute for Human-Centered Artificial Intelligence \nat Stanford University, explained that a résumé-screening system developed by Amazon using machine \nlearning was found to be discriminatory because the system was trained on data from résumés Amazon had \nalready received, the overwhelming majority of which were from men. The system was never used to make \nhiring decisions. \n8 \nGrand View Research, Facial Recognition Market Size, Share & Trends Analysis Report by Technology (2D, \n3D, Facial Analytics), by Application (Access Control, Security & Surveillance), by End-use, by Region, and \nSegment Forecasts, 2021 - 2028. \n9 \nStanford University, Human-Centered Artificial Intelligence Institute, Artificial Intelligence Index Report \n2022; See also: Nestor Maslej, Brief to ETHI Committee – Study on the Use and Impact of Facial Recognition \nTechnology, 9 June 2022. \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n11 \nalso growing. Our estimates also show that FRT is the 12th-most funded area out of 25 \nAI focus areas. \nSeveral other witnesses mentioned the already widespread use of FRT. For example, \nDr. Watkins said that FR is used on Uber drivers, Amazon delivery drivers and home care \nproviders to electronically verify each visit. Many police departments across the United \nStates are using FRT, except in cities with bans or moratoriums on it. Ms. Wang said FRT \nis used by interviewing platforms like HireVue. Rob Jenkins, a professor of psychology at \nthe University of York in the United Kingdom, said that a number of countries use FRT at \nborder controls and in other processes, like passport renewal. Specific uses of FRT in \nCanada will be discussed in Chapter 2 of this report. \nDiane Poitras, president of the Commission d’accès à l’information du Québec (CAI), said \nthat, in addition to identity verification, the term FR is sometimes used to designate \nderivatives of the technology, which can be used for corporate purposes, in shopping \ncentres for example, where the goal is not to identify individuals but rather their \ncharacteristics, like age, sex or time spent window-shopping.10 \nSanjay Khanna, a strategic advisor and foresight expert, for his part, alluded to a future \nin which FRT could be used for sentiment analysis, for example for commercial \nmanipulation purposes, or embedded into security robots or gambling. However, \nMs. Wang said the following with respect to that type of FRT: \nIt’s worth noting here that there is also lots of pseudoscience on other kinds of facial \nrecognition tasks, such as gender prediction, emotion prediction, and even sexual \norientation prediction and criminality prediction. There has been warranted backlash \nand criticism of this work, because it's all about predicting attributes that are not \nvisually discernible. \nBenefits of Facial Recognition Technology \nMany witnesses acknowledged that some uses of FRT could benefit society.11 For \nexample, Ms. Piovesan said that FRT can facilitate quick and secure payment at \ncheckout, or help save a patient’s life. She said that FRT is used in health care to monitor \npatients and make sure their condition does not change. She said that FRT can be useful \n \n10 \nETHI, Evidence, Diane Poitras. \n11 \nETHI, Evidence, Alex LaPlante; ETHI, Evidence, Carole Piovesan; ETHI, Evidence, Françoys Labonté; ETHI, \nEvidence, Daniel Therrien; ETHI, Evidence, Sanjay Khanna; ETHI, Evidence, Owen Larter; ETHI, Evidence, \nRob Jenkins. \n \n12 \nfor verifying a person’s identity to access their bank or their phone. FRT can also be \nuseful for conducting financial transactions. \nFrançoys Labonté, chief executive officer of the Computer Research Institute of \nMontréal, said that, generally speaking, “people are in favour of using facial recognition \ntechnology for specific clearly-stated applications when it’s easy to understand the \nbenefits and how the data will be used.” \nMs. McPhail mentioned the convenient and widespread use of facial verification to \nunlock phones, which, with appropriate built-in protections, may pose relatively little \nprivacy risk. \nOwen Larter, director responsible for artificial intelligence public policy at Microsoft, said \nthat FRT can have a lot of benefits. Among these, he too mentioned identity verification \nusing FR for a person’s phone or computer. He noted the beneficial applications of FRT in \nthe accessibility context, stating that some organizations are doing research on how to \nuse FR to help people who are blind or with low vision better understand and interact \nwith the world around them. One such project, called Project Tokyo, uses a headset so \nthat an individual who is blind can scan a room and identify people who have consented \nto be part of their FR system, enabling them to identify that person and start a \nconversation. Mr. Larter also mentioned an application that aims to help people with \nAlzheimer’s or similar diseases recognize friends and loved ones.12 \nMr. Khanna said that FRT can be beneficial when used to prevent industrial accidents, for \nexample by preventing employees from falling asleep or not being alert to a lack of \nattention. \nDubi Kanengisser, senior advisor in strategic analysis and governance for the Toronto \nPolice Services Board (TPSB), said that FR can be another tool used by law enforcement \nto carry out their duties of identifying perpetrators and victims. \nDaniel Therrien, former privacy commissioner of Canada, said that FR can be used for \nserious crimes, such as missing children, and for other compelling state purposes, such \nas in the border context to ensure that people of concern can be identified without \nimpeding the flow of travellers into the country. \nKristen Thomasen, a law professor at the University of British Columbia, emphasized, \nhowever, that privacy is a social good that benefits everyone. That includes women and \nchildren who are often cited in the narrative that one of the beneficial uses of FR is to \n \n12 \nMicrosoft, Project Tokyo. \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n13 \nprotect marginalized or victimized groups in certain contexts such as human trafficking \nor child abuse. She agreed that these beneficial uses of FRT should be acknowledged \nwhile nuancing that narrative considerably, as the erosion of privacy as a social good will \nalso harm women and children. She stated that FRT consolidates and perfects \nsurveillance, and that more perfect surveillance means greater privacy harm and \ninequity. \nProf. Thomasen stressed that FRT is not inevitable and that pointing to some beneficial \nuse cases should not be sufficient to limit thinking around the potential harms that can \narise from more widespread use of the technology. Ana Brandusescu, an artificial \nintelligence governance expert, and Ms. Poitras also cautioned against trivializing the \nrisks that FRT poses because of its popularity or because it is convenient. \nConcerns About Facial Recognition Technology \nMisidentification and Algorithmic Bias \nThe biggest concern with the use of FRT is the potential for misidentification. For \nexample, Cynthia Khoo, a research fellow at the Center on Privacy and Technology at \nGeorgetown Law School in Washington, D.C., and with the Citizen Lab at the University \nof Toronto, said that researchers have found that FRT is up to 100 times more likely to \nmisidentify Black and Asian individuals. It misidentifies more than one in three darker-\nskinned women, but is 99% accurate for white men. However, Ms. Wang noted that \nalthough models developed in Asia also have lots of biases “[t]hey are just a different set \nof biases than models that have been developed by Canadians or Americans”. \nMs. Brandusescu presented statistics similar to those provided by Ms. Khoo: \nFRT is better at distinguishing white male faces than Black, brown, indigenous and trans \nfaces. We know this from groundbreaking work by scholars like Joy Buolamwini and \nTimnit Gebru. Their study found that: \n… darker skinned females are the most misclassified group (with error rates of up to \n34.7%). The maximum error rate for lighter-skinned males is 0.8%. \nWitnesses referred to a study conducted by the U.S. National Institute of Standards and \nTechnology (NIST), which found that some algorithms perform worse for certain \n \n14 \ndemographic groups.13 The report found that, for algorithms developed in the U.S., false \npositive rates are highest for Asians and African Americans compared to Caucasians. For \ndomestic law enforcement images, the highest false positive rates were for Indigenous \npeoples. The report also found false positive rates to be higher for women than men, \nand higher for the elderly and for children. \nMs. Piovesan also raised concerns about accuracy and bias in system outputs, unlawful \nand indiscriminate surveillance and black box technology that is inaccessible to \nlawmakers, restricting freedom and putting at risk fundamental values as enshrined in \nthe Canadian Charter of Rights and Freedoms. Alex LaPlante, senior director of product \nand business development at Borealis AI, made similar comments, stating: \n[I]f we don’t take care to adequately assess the application, development and \ngovernance of AI, it can have adverse effects on end-users, perpetuate and even amplify \ndiscrimination and bias towards racialized communities and women, and lead to \nunethical usage of data and breaches of privacy rights. \nDr. Watkins said that technologies such as AI, machine learning and algorithmic \ntechnologies based on data gathered over years and decades reflect human biases like \ninstitutional racism and sexism. These processes are “very conservative and very old-\nfashioned, and they are perpetuating the biases that we, as a society, ought to figure out \nhow to…get past.” \nHowever, some witnesses said that FRT has come a long way. For example, Prof. Jenkins \nsaid that impressive progress has been made in the past five years in how well these \nsystems can identify faces. Mr. Maslej noted: \nIn 2017, some of the top-performing facial recognition algorithms had error rates \nanywhere from roughly 20% to 50% on certain FRVT [Facial Recognition Vendor Test] \ndatasets. As of 2021, none has posted an error rate greater than 3%, with the top-\nperforming models registering an error rate of 0.1%, meaning that for every one \nthousand faces, these models correctly identify 999. \nDespite any progress, a few witnesses said that FRT would still raise concerns, even if it \nworked optimally.14 For example, Ms. Khoo said that, even if FRT worked perfectly, it \n \n13 \nETHI, Evidence, Alex LaPlante; International Civil Liberties Monitoring Group, Brief to the ETHI Committee – \nStudy on the Use and Impact of Facial Recognition Technology, 13 April 2022 [ICLMG Brief], p. 4; National \nInstitute for Standards and Technology (NIST), Face Recognition Vendor Test (FRVT) Part 3: Demographic \nEffects, December 2019. \n14 \nETHI, Evidence, Kristen Thomasen; ETHI, Evidence, Tim McSorley; ETHI, Evidence, Cynthia Khoo; ETHI, \nEvidence, Brenda McPhail; ETHI, Evidence, Angelina Wang. \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n15 \nmight be used to the detriment of social groups that fall along historical lines of systemic \noppression. Ms. McPhail made similar comments: \n[I]f the technology is fixed and if it becomes more accurate on all faces across the \nspectrums of gender and race, it may become even more dangerous. Why? It's because \nwe know that in law enforcement contexts, the surveillance gaze disproportionately falls \non those same people. We know who often suffers discrimination in private sector \napplications. Again, it’s those same people. In both cases, a perfect identification of \nthese groups or members of these groups who already experience systemic \ndiscrimination because of who they are and what they look like carries the potential to \nfacilitate simply more perfectly targeted discriminatory actions. \nProf. Thomasen said that facial surveillance must be considered within its historical \ntrajectory, which emerged from eugenics and white supremacist ideologies. She also \ncautioned that personal use of facial surveillance can be damaging with respect to \nharassment, doxing and other forms of technology-facilitated violence. \nFrom a technical perspective, Ms. Wang explained that, because machine learning \nmodels try to identify patterns in data, they frequently amplify biases that exist in that \ndata. She illustrated this point using the following example: \n[I]n predictive policing, if communities of colour and different neighbourhoods with \nhigher proportions of Black citizens may have higher levels of crime, then predictive \npolicing models may over-report those communities in the future to be more likely to \nhave crime, even if that is not true, and will over-amplify this compared to the base rate \nof what the correlation actually is. \nShe also pointed out that, even if bias problems across demographic groups were \nresolved, two problems would remain: brittleness (known ways that bad actors can \nmanipulate FR models to circumvent and trick them) and interpretability: it is extremely \ndifficult to discover the precise set of rules the model is using to make these decisions.15 \nOther Concerns \nTim McSorley, national coordinator of the International Civil Liberties Monitoring Group \n(ICLMG), raised three main concerns about FRT: biased and inaccurate algorithms \nreinforce systemic racism and racial profiling; facial recognition allows for indiscriminate \nand warrantless mass surveillance; and the lack of regulation, transparency and \naccountability from law enforcement and intelligence agencies in Canada.16 \n \n15 \nETHI, Evidence, Angelina Wang. \n16 \nSee also: ICLMG Brief. \n \n16 \nPatricia Kosseim, Ontario’s Information and Privacy Commissioner, said that, with \nrespect to the use of FRT, the greatest concern of commissioners across Canada is mass \nsurveillance, whether done by a third-party private sector company on behalf of the \npolice or by the police service itself. Ms. McPhail noted that in addition to equality \nrights: \n[T]ools that could allow ubiquitous identification would have negative impacts on a full \nrange of rights protected by our Canadian Charter of Rights and Freedoms and other \nlaws, including freedom of association and assembly, freedom of expression, the right to \nbe free from unreasonable search and seizure by the state, the presumption of \ninnocence … and ultimately rights to liberty and security of the person. \nAnother concern with FRT is that people are not always aware that their face is part of a \ndataset used by FRT. The Clearview AI case is an example of such a situation. Ms. Wang \nsaid that the “individuals whose faces are included in this dataset generally do not know \ntheir images were used for such a purpose, and may consider this to be a privacy \nviolation.” \nAccording to Prof. Jenkins, when an image is included in FRT algorithms, it becomes \nimpossible to eliminate the influence of that image on the algorithm. He also explained \nthe concept of intrapersonal variability: each of us has one face, which has its own \nappearance that constantly varies. This principle is an important factor in facial \nrecognition since it involves not only the natural aging of the face, but also factors such \nas the angle, lighting, or a person’s facial expression. Intrapersonal variability causes a lot \nof variation, which is difficult to overcome when using FRT. \nProf. Jenkins also noted that human oversight can catch egregious errors. However, he \nsaid that human face recognition is not infallible and can therefore also introduce errors \ninto the system.17 \nMs. Poitras brought up the privacy risks of biometric databases, noting that databases \ncreated for one purpose may be used for other purposes without an individual’s \nknowledge or an adequate assessment of the risks associated with those other \npurposes. Witnesses also raised concerns about security weaknesses and data \nbreaches.18 \n \n17 \nSee also: Rob Jenkins, Brief to the ETHI Committee – Study on the Use and Impact of Facial Recognition \nTechnology, 4 April 2022. \n18 \nLigue des droits et libertés, Brief to the ETHI Committee – Study on the Use and Impact of Facial Recognition \nTechnology, 15 April 2022, p. 6 [Ligue des droits et libertés Brief]; Tessono Brief, p. 6. \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n17 \nFinally, for many witnesses, the fact that facial traits are permanent, and cannot be \nchanged like passwords, makes FRT particularly invasive.19 \nCHAPTER 2: USES AND RELATED RISKS \n“[Facial recognition technology] cannot bear the legal \nand moral responsibility that humans might otherwise \nabdicate to it over vulnerable people’s lives and freedom.” \nCynthia Khoo, research fellow,  \nwho appeared before the Committee on 21 March 2022 as an individual. \nBeyond general observations with regards to FRT, witnesses also commented on specific \nuses of FRT. \nDr. Watkins said that the use of FRT by government organizations, such as police \nagencies, border authorities and the government in general, is high risk. She said that, \nnot only are these technologies unreliable, but they also presume that social constructs, \nlike race and gender, are machine-readable in a person’s face which, which in her \nopinion, “is simply untrue.” She added that AI technologies, like Clearview AI, are not yet \naccurate enough and should not be used in high-risk scenarios, where lives and \nlivelihoods are on the line. \nOne high-risk scenario discussed by witnesses was the use of FRT by police forces. \nUse of Facial Recognition by Police Forces \nCriticism \nAccording to Ms. Khoo, one of the key problems with law enforcement use of FRT is the \nlack of transparency. The public often learns about the technology’s use through media, \nleaked documents and freedom of information requests. Mr. McSorley and Ms. McPhail \nalso noted the lack of transparency from law enforcement in Canada. Ms. McPhail \ndescribed the situation in Canada as “a real crisis of accountability when it comes to \npolice use of these technologies.” \n \n19 \nETHI, Evidence, Angelina Wang; ETHI, Evidence, Daniel Therrien; ETHI, Evidence, Diane Poitras. \n \n18 \nProf. Jenkins criticized the reliability of FRT. He compared other identification methods \nused by police forces, such as fingerprinting, to FR to demonstrate the existence of more \nreliable techniques, in certain circumstances. He explained that other more traditional \nmethods do not have the same number of technical problems that FRT presents: \nchanges in facial appearance, lighting conditions, the distance from the face to the \ncamera lens, and so on. \nProf. Jenkins said “[o]ne of the main concerns is mistaken identity and just the idea that \nan innocent person could be apprehended, accused and even sentenced for a crime they \ndid not commit.” He also recognized that it is important to “avoid the opposite error of \nfailing to apprehend someone who could be a great danger to other people.” \nMs. Khoo, Ms. Brandusescu, Ms. McPhail and Ms. Wang each mentioned cases of \nmisidentification of Black men in the U.S. that led to wrongful arrests. Ms. Khoo gave \nRobert Williams, Nijeer Parks and Michael Olivier as examples: \nAll three are Black men who were wrongfully arrested by police relying on facial \nrecognition technology. They have endured lost jobs, traumatized children and broken \nrelationships, not to mention the blow to personal dignity. These are the human costs of \nfalse confidence in, and unconstitutional uses of, facial recognition technology. \nHowever, Ms. McPhail said that she was not aware of any examples where the \nmisidentification of an individual led to criminal charges in Canada. She said that this is \nbecause police forces in Canada have been cautious and measured in adopting this \ntechnology and are using it in relatively limited ways. \nMs. Khoo said that racial justice activists, whom she and her colleagues talked to in the \ncontext of the research conducted for the Citizen Lab report, consider the use of \nalgorithmic technologies by police to be 21st-century state violence: before it was done \nwith pen and paper, now it is done with computers and algorithms.20 \nMs. Brandusescu said that there is systemic racism in policing in Canada, noting that \nthis was acknowledged by the House of Commons Standing Committee on Public Safety \nand National Security in a 2021 report.21 In her view, FRT exacerbates systemic racism. \n \n20 \nThe Citizen Lab, Kate Robertson, Cynthia Khoo and Yolanda Song, To Surveil and Predict – A Human Rights \nAnalysis of Algorithmic Policing in Canada, 1 September 2020. \n21 \nStanding Committee on Public Safety and National Security (SECU), Systemic Racism in Policing in Canada, \nJune 2021. \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n19 \nMs. McPhail said that police use of FRT with mugshot databases is inherently \nproblematic since they have their own issues of bias and discrimination. The ICLMG \nmade similar comments.22 \nSharon Polsky, president of the Privacy and Access Council of Canada, consulted with \npolice forces on the use of FR and believes that police officers, like most Canadians, do \nnot really understand compliance requirements or what FRT can actually do.23 \nRisk of Mass Surveillance by Police Forces \nSome witnesses raised the possibility that police activities lead to mass surveillance.24 \nMr. McSorley gave the specific example of the RCMP: \nFor example, the RCMP scrape information about individuals online and keep those in \ndatabases. We know they have been doing that. This is beyond facial recognition, but \nthey would argue they have a right to collect that information, whereas others have \nbeen challenging it as we have, saying that it’s a form of mass surveillance that needs to \nbe regulated. \nGordon Sage, director general of the RCMP’s Sensitive and Specialized Investigative \nServices, said that he does not believe that the mere use of FRT constitutes mass \nsurveillance, even with respect to the use of Clearview AI’s technology, which matched \nimages against a database of three billion publicly sourced images. \nPaul Boudreau, acting deputy commissioner of the RCMP’s Specialized Policing \nServices, said that the RCMP does not use FRT for active surveillance or the capturing of \nmass protests. However, Mustafa Farooq, president and CEO of the National Council of \nCanadian Muslims (NCCM), said his organization gets calls all the time from people \nundergoing surveillance by the Canadian Security Intelligence Service (CSIS) or the RCMP \nat rallies or protests. Mr. Sage of the RCMP denied this allegation, assuring the \nCommittee that the RCMP does not use any FRT for mass surveillance. He added that \nthe only FRT used by the RCMP was Clearview AI’s, but that this use stopped in \nJuly 2020. \n \n22 \nICLMG Brief, p. 4. \n23 \nSee also: Privacy and Access Council of Canada. Facial Recognition Use by Law Enforcement in Canada: \nRealities, Reservations, and Recommendations, 15 October 2021. \n24 \nETHI, Evidence, Brenda McPhail; ETHI, Evidence, Tim McSorley. \n \n20 \nMr. Therrien said that he had no reason to doubt the RCMP’s statement that it does not \nconduct mass surveillance or use FRT to do so, although he found their definition of the \ncircumstances under which they use it rather ambiguous. \nColin Stairs, chief information officer of the Toronto Police Service (TPS), assured the \nCommittee that the TPS does not conduct mass surveillance, does not take photos of \nprotesters as a practice, and therefore does not run these types of photos through FR. \nHe added that the TPS uses FRT as an investigative tool, not as a surveillance or \nreconnaissance tool that would infringe on privacy. For example, he explained the TPS’ \nuse of FR: \nWhat we are doing is taking crime scene photos gathered from cameras that would be \nrecording the street regardless, taking a still from that and comparing it to the mug shot \ndatabase, which is very similar to witnesses giving testimony. This is not a significant \nchange. \nSimilarly, Dr. Kanengisser said that anything that falls under “mass surveillance” is an \nunreasonable use of FRT. For example, tracking people en masse indiscriminately would \nbe unacceptable to the TPSB, as would the use of any technology that can be shown to \nbe inaccurate, leading to significant misidentification and potential harm. For example a \nperson getting arrested because they were misidentified by a software and that was not \nconfirmed by a human would be unacceptable. \nThe following sections further discuss the use of FRT by the RCMP and TPS, including the \nuse of Clearview AI’s FRT. \nUse by the Royal Canadian Mounted Police \nAccording to Mr. McSorley, the RCMP has used different forms of FR over the past \n20 years without any public acknowledgement, debate or clear oversight. Mr. Boudreau \nsaid, “We’ve been using facial recognition within the organization [the RCMP] for a very \nlong time” but that, when it comes to new FRT “such as Clearview, we are not using that \ntype of technology.” Mr. Sage also said that the RCMP does not currently use FRT. The \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n21 \nRCMP clarified its testimony regarding its use of FRT in a letter to the Committee in July \n2022.25 \nHowever, the RCMP has admitted to using Clearview AI’s FRT in the past. Mr. Sage and \nMr. Boudreau confirmed that two licences were purchased in October 2019 and used \nuntil July 2020, when Clearview AI withdrew from the Canadian market. \nMr. Sage said that he believed that a licence was first obtained by an investigator \nworking for the National Child Exploitation Crime Centre (NCECC). He added that the \ndirector general at the time was not aware of the purchase when it was made. He also \nnoted that the employee in question was never investigated. Mr. Boudreau also \nconfirmed that no RCMP officer was reprimanded regarding the use of Clearview \nAI’s FRT. \nMr. Boudreau explained that the RCMP is constantly looking at new technologies, \nwhether it is FR or other types of technologies, so different divisions look at and \nevaluate new technologies. However, he said that, when the RCMP learned that a \nlimited number of RCMP programs and services had begun using Clearview AI, an \ninternal investigation was launched. \nMr. Sage said that because no policy was in effect at the time the licence was obtained, \n“members on the ground were able to obtain licences as they saw fit.” Mr. Sage added \nthat no analysis of the technology’s compliance with the Charter took place at the time. \nThe RCMP also confirmed that no ethics review was done before using Clearview AI’s \nFRT.26 Roch Séguin, director of the Strategic Services Branch, Technical Operations, said \nthat the RCMP had approached the Department of Justice regarding the use of FRTs in \nits investigations only once, but it was internal to the RCMP. \nMr. Sage explained that Clearview AI’s FRT was tested by a lot of RCMP members, using \neither their own photos, from profiles and social media, or photos of celebrities. They \n \n25 \nRoyal Mounted Police of Canada, Letter to the Committee, 21 July 2022. The letter clarifies the RCMP’s \ntestimony on the use of Facial Recognition Technology (FRT). It indicates that the RCMP uses certain FRT \nthat had not previously been highlighted as such, namely Spotlight and Traffic Jam. Both tools use facial \nrecognition, and other components, to help law enforcement identify victims of sexual exploitation, human \ntrafficking and missing persons who are at risk of exploitation, by conducting searches on open websites. \nNone of these tools have yet been evaluated by the National Technologies Onboarding Program. The letter \nalso provides a list of RCMP use of FRT, a list of likely future use of new technologies and describes the \napproval process for purchasing licences from Clearview AI. \n26 \nRoyal Canadian Mounted Police, Confidential letter to the Committee, 3 June 2022. \n \n22 \nfound that the technology was not always effective and had some identification \nproblems. As a result, it became one of many tools requiring human intervention. \nMr. Sage said that FRT was used in RCMP investigations on only three occasions: the \nNCECC used it on two occasions to identify victims of serious crime and provide \nsafeguards to protect the victims who were located in Canada. It was used on a third \noccasion to track a fugitive abroad in cooperation with other police forces. He assured \nthe Committee that the RCMP’s use of FRT has never resulted in prosecutions in \nCanada.27 Mr. Boudreau also said that human intervention must always be used when \nanalyzing results. \nHowever, the OPC report on the use of Clearview AI’s FRT found that only 6% of \nthe searches by Clearview AI appeared to be linked to NCECC victim identification, and \napproximately 85% were not accounted for at all by the RCMP.28 According to Mr. Sage, \n6% of the searches were for the three cases mentioned above, while the remaining 85% \nwere used to test the technology. \nMr. Therrien explained that, as a result of his investigation, the OPC found that the \nRCMP had not taken any measures to verify the legality of Clearview AI’s collection of \ninformation and lacked any system to ensure that new technologies were deployed \nlawfully. The OPC ultimately determined that Clearview AI’s use of FRT was unlawful \nbecause it relied on the illegal collection and use of facial images by its business partner. \nThe OPC also found that there were “serious and systemic failings by the RCMP to \nensure compliance with the Act before it collected information from Clearview and, \nmore broadly, before novel collection of personal information in general.”29 Mr. Therrien \nnoted that the words used in the report refer to the fact that at the time of the \ninvestigation the RCMP did not have a verification and approval process in place to \nensure that, when new technology is used by its officers, the technology respects the \nlaw and privacy rights. \nMr. Therrien explained that the RCMP disagreed with the OPC’s findings that it had \nfailed to comply with section 4 of the Privacy Act by using Clearview AI technology. He \nnoted that the RCMP argued that the section does not explicitly require a government \n \n27 \nFor example, Gordon Sage said that FRT was successful in identifying and finding a child who was a victim of \nsexual exploitation when traditional methods over the past 9–10 years had failed. \n28 \nSpecial OPC report on the RCMP, para. 18. \n29 \nIbid., para. 87. \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n23 \ninstitution to verify the legality of its business partner’s practices before the public \nsector uses the information.30 \nMr. Therrien agreed that the above requirement is not explicit, but said that it exists \nimplicitly under section 4 of the Privacy Act. Otherwise, a federal institution could, \nthrough contracting with the private sector, engage in practices it cannot engage in \ndirectly. He recommended that the ambiguity in the Privacy Act be removed by explicitly \nrequiring all government institutions to ensure that what they are buying is lawful when \nthey contract with the private sector. \nMr. Boudreau and Mr. Sage confirmed that the RCMP does not agree with all of the \nfindings of the OPC’s report, but supports its recommendations. \nMr. Therrien noted that, despite the RCMP’s position, it was making good progress, in \ncooperation with the OPC, to have a better verification system for new technologies, \nbe it FR or other new technologies. During his appearance, he said that the RCMP \nwas unlikely to be able to implement all of the OPC’s recommendations by the \nrecommended 12-month deadline but said he believed the RCMP was making a \ngenuine effort. \nMr. Séguin said that the OPC’s recommendations led to a national technologies \nonboarding strategy in March 2021: the National Technologies Onboarding \nProgram (NTOP). Since then, the RCMP has made significant progress in implementing \nthe NTOP to ensure that technologies are assessed before being used in any operation \nor investigation. He said that the NTOP was expected to be in place by June 2022, within \nthe 12 months recommended by the OPC, but that staff training may not have been \ngiven by that time.31 \nMr. Séguin described the key pillars of the NTOP: \nWith regard to the key pillars for the national technology onboarding program, or \nstakeholder outreach and partnership, which includes the training, obviously there’s a \npolicy review in development to identify all gaps with existing policy and to modify and \nupdate new ones. There’s a technology assessment portion, where we built a full intake \nprocess through a series of questionnaires. Also, we’re implementing a technology \n \n30 \nClearview AI challenged the OPC’s findings and the orders made by the provincial commissioners. The \nfederal commissioner does not have the power to make orders. See: ETHI, Evidence, Brenda McPhail; and \nETHI, Evidence, Diane Poitras. In Quebec, Clearview AI is challenging the decision of the Commission d’accès \nà l'information du Québec (CAI) in court, including the CAI’s jurisdiction over a U.S. company. \n31 \nRoyal Mounted Police of Canada, Letter to the Committee, 21 July 2022, p. 3. As of 21 July 2022, 31 new \ntechnologies had been submitted for review by the National Technologies Onboarding Program (NTOP), \nwhich is in its preliminary operational state. The letter describes the NTOP in more details. \n \n24 \ninventory for awareness oversight. The last component is going to be public awareness \nand transparency. \nMr. Boudreau said that the NTOP provides an opportunity to look at all new technologies \nfrom a legal, ethical and privacy perspective. He added that the RCMP believes that the \nuse of FR “must be targeted, time-limited and subject to verification by trained experts” \nand “should not be used to confirm an identity, but rather only be considered as an \ninvestigational aid where the results must be confirmed, again, by human intervention.” \nAccording to RCMP officials, the RCMP’s partners will have to follow its policies.32 \nMr. Séguin also assured the Committee that “[f]rom a public awareness and \ntransparency piece, it is built in as part of our communications strategy to relieve the \ncategories of technology that the RCMP will be leveraging in the future.” \nMr. Sage added that the NTOP assesses the risks and ethical issues of the technology, \nand includes a privacy assessment. He said that as part of the RCMP’s efforts to develop \nnew ways forward, the RCMP has “a member located within [the OPC’s] office, and we \nare asking for one of their employees to be with our office in order to strengthen that \nknowledge and relationship.” \nMr. Boudreau added that, when the RCMP looks at technologies such as FRT, they have \nto be looked at through the lens of a legal, privacy, gender-based analysis and bias \nperspective, and have human intervention as well. \nAs for the future use of FRT by the RCMP, Mr. Sage said it was unfortunate that FRT \ncannot be used in child exploitation cases to identify victims. He said it is an urgent file. \nHe said he is waiting on a decision from national technical operations, as required by the \nNTOP process, to do an assessment of the use of FRT. Once the assessment is done, he \nhopes to have permission to use FRT for victims at risk. \nDespite the actions taken by the RCMP, Rizwan Mohammad, an advocacy officer with \nthe NCCM, said that the NCCM is shocked by the blasé attitude the RCMP has taken in \napproaching the issue of its use of Clearview AI. He noted that the RCMP had initially \ndenied using Clearview AI’s FRT but then confirmed it had been using the software, \nclaiming that the use of FRT was not widely known within the RCMP. \nThe RCMP’s use of other FRT was also raised by some witnesses. For example, \nMr. McSorley said that the RCMP has contracted with IntelCenter, a U.S.-based private \n“terrorist facial recognition” system that provides “access to facial recognition tools and \n \n32 \nETHI, Evidence, Gordon Sage; ETHI, Evidence, Roch Séguin. \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n25 \na database of more than 700,000 images of people associated with terrorism.” The \ncompany says it acquires these images from scraping online, like Clearview AI. He said \nthat law enforcement’s use of IntelCenter is of great concern as it adds the extra stigma \nof saying that they know these people are associated with terrorism, with no oversight \nin terms of how they came to that determination. That information is then used by law \nenforcement. \nHowever, Mr. Sage said that the IntelCenter software was acquired by the RCMP on an \ninternal trial basis only. It was not tested or used in any national security investigation or \nother operational capacity. He added that, in March 2018, when the RCMP learned that \nthe IntelCenter service software was not approved for operational use, “its use by \nE Division was discontinued.” \nLastly, Mr. Sage said that Project Arachnid, a program run by the Canadian Centre for \nChild Protection in partnership with the NCECC, does not use FRT. It uses a hashtag \nsearch, which is the DNA of an image, to crawl the Internet.33 \nUse by the Toronto Police Service \nColin Stairs confirmed that the TPS uses FRT to compare probe photos uncovered in \ninvestigations against photos in its Intellibook, the TPS’ mugshot database. Mr. Stairs \nassured the Committee that the body camera images used by the TPS do not go into the \nmugshot database and that there is “no connection between the body-worn cameras \nand the Intellibook system, no automated connection.” He said the TPS operates under \nthe Identification of Criminals Act and therefore uses only mugshots, which come from \narrests and processing. Clearview was an anomaly in that regard. The TPS does not use \npublicly sourced facial images in its facial recognition program. \nMr. Stairs acknowledged that there is a known set of issues around face analysis in \ndifferent training sets. He explained that the TPS selected the FRT it uses on the basis of \nminimizing racial bias, while recognizing that there are biases embedded into \nphotographic systems (e.g., biases towards lighter faces compared to darker faces). For \nthat reason, the TPS uses a “hurdle rate” below which the TPS does not consider it a \nmatch. A match is not considered an identity: the identity has to be corroborated by \nother methods. \n \n33 \nCanadian Centre for Child Protection, Project Arachnid; See also: ETHI, Ensuring the Protection of Privacy \nand Reputation on Platforms such as Pornhub, June 2021. The report discusses Project Arachnid. \n \n26 \nMr. Stairs said that FRTs can be helpful when an unknown witness or subject is involved \nin a violent crime or a significant issue. However, their usefulness is limited by the scope \nof the TPS’ mugshot database and the restrictions imposed by the Criminal Code and the \nCanadian Charter of Rights and Freedoms. \nMr. Stairs confirmed that the TPS’ use of FRT is always accompanied by human analysis \nby the forensic identification service, stating that “there is a technician who takes the \nimage, runs it into the system and looks at the results.” He said he believes that any \ninformation related to the TPS’ use of FRT in an investigation is shared after an arrest \nwith the court or the defendant. \nIn February 2022, the TPSB adopted a Use of Artificial Intelligence Technology Policy (AI \npolicy).34 Dr. Kanengisser explained that, under the AI policy, the use of FRT or other \nbiometric technology is considered “high risk.” Considerable reviews in advance of \nadoption and deployment of the technology are therefore required.35 Follow-up on the \ntechnology is also done over at least two years to examine any impact, including any \nunintended consequences. \nDr. Kanengisser said that the AI policy includes guiding principles for deciding whether or \nnot a technology should be approved, which include issues of fairness and reliability, the \nlegality of the use, and the requirement for human intervention at all times. \nMr. Stairs said that the TPS is drafting the procedure that will implement the AI policy \nadopted by the TPSB, and that consultations similar to those conducted to develop the \npolicy will be held with stakeholders. In terms of hoped-for outcomes, he said that part \nof the problem is “insufficient visibility and guidance to frontline officers on how they \nshould approach new technologies.” As a result, the TPS is looking to create a framework \nthat allows it to filter and indicate to the TPSB and the public the types of technologies it \nintends to use and why. \nMr. Stairs explained that, under the AI policy, the levels of risk for evaluating \ntechnologies are extreme risk, high risk, medium risk, low risk, and very low risk. \nExtreme risk would be banned. He added that a high or extreme risk level must involve \nhuman intervention. He said that the AI policy also requires that all technology must be \n \n34 \nToronto Police Services Board, Use of Artificial Intelligence Technology Policy, 22 February 2022. \n35 \nETHI, Evidence, Opening Remarks, Dubi Kanengisser. The Toronto Police Service (TPS) needs to demonstrate \na real need and a mitigation plan to address any risks of bias or infringement of privacy or other rights in \norder to adopt a new “high-risk” tool and ensure a governance structure that allows for effective auditing. \nThe policy also places an emphasis on training for TPS members. \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n27 \nposted and evaluated under the framework, except for low and very low risk \ntechnologies. Otherwise, the load on the TPS would be very high. \nMs. Kosseim said that the Office of the Information and Privacy Commissioner of Ontario \nwas consulted on the AI policy. She said that, while not all of her office’s \nrecommendations were adopted within the policy, they can be adopted within the \nprocedures that implement it. Vance Lockton, a senior technology and policy advisor in \nthe Commissioner’s office, said, for example, that one of the Commissioner's \nrecommendations would be to include in the procedures better definitions of risk levels \nand how oversight of AI used by the TPS will be exercised. \nAccording to Ms. Thomasen, the TPS’ AI policy still has some weaknesses. For example, it \nstill treats algorithmic policing technologies as inevitable—as a net benefit whose risks \ncan be mitigated. She believes that this is not the right framework to address these \ntechnologies given the harms they can cause and the social context in which they are \nintroduced. \nUse of Facial Recognition by Other Federal Agencies \nIn 2020, a group of 77 privacy, human rights and civil liberties advocates, including the \nICLMG, wrote a letter to the Minister of Public Safety calling on him to ban all use of \nfacial recognition surveillance by federal law enforcement and intelligence agencies.36 \nFurther to the letter Mr. McSorley attended a listening session with the director of policy \nin the Minister’s office, where he was told that the Canada Border Services \nAgency (CBSA) does not use real-time FR. Mr. McSorley added that no information was \nshared about CSIS’s use of the technology, and no clear commitment was given from the \nMinister’s office to take further action. \nReferring to the patchwork of legislation around FRT, Mr. McSorley said: \nThe lack of discussion and the lack of forthcomingness from federal agencies to discuss \ntheir use of facial recognition technology is what raises these deep concerns that they \ncould be engaging in forms of surveillance that are unlawful or which otherwise would \nbe considered unlawful, but are doing so because of this patchwork of legislation. \nMr. McSorley said that CSIS has refused to confirm whether or not it uses FRT in its work, \nstating that it has no obligation to do so. \n \n36 \nInternational Civil Liberties Monitoring Group, Open Letter: Canadian Government Must Ban Use of Facial \nRecognition by Federal Law Enforcement, Intelligence Agencies. \n \n28 \nMr. Mohammad said that a number of national security and policing agencies, as well as \nother government agencies, have said that their use of surveillance was done in ways \nthat were constitutionally sound and proportionate, when that was not the case, as \ndemonstrated by the cases of Maher Arar, Abdullah Almaki and \nMohamedou Ould Slahi.37 He said: \nThe same agencies that lied to the Canadian people about surveilling Muslim \ncommunities are coming before you now to argue that while mass surveillance will not \nbe happening, FRT can and should be used responsibly. \nMr. Farooq brought up the example of a recent Federal Court decision that criticized CSIS \nfor its habit of trying to mislead the court.38 He noted that the government is appealing \nthe decision. He added that what is going to be done to challenge national security \nagencies when they mislead people remains an open question. \nUse of Facial Recognition by Border Authorities \nIn September 2020, the Samuelson-Glushko Canadian Internet Policy and Public Interest \nClinic (CIPPIC) released a report about FR and its use at the border.39 Tamir Israel, a \nlawyer with the CIPPIC, outlined the report’s key findings: \n[F]acial recognition is being adopted at the border without due consideration for the \nharms it would cause, without much external oversight and often without regard to \nexisting policies, such as the Treasury Board's policy on artificial intelligence, where you \nare supposed to bring in external guidance when adopting intrusive technologies like \nthat. \nThen, once it is adopted, it often gets repurposed very quickly for reasons beyond the \nnarrow reasons of the context in which it was developed. \n \n37 \nSee: SECU, Review of the Findings and Recommendations of the Internal Inquiry into the Actions of \nCanadian Officials in Relation to Abdullah Almalki, Ahmad Abou-Elmaati and Muayyed Nureddin (Iacobucci \nInquiry) and the report from the Commission of Inquiry into the Actions of Canadian Officials in Relation to \nMaher Arar (O’Connor Inquiry), June 2009. \n38 \nCanadian Security Intelligence Services Act (CA) (Re), 2020 FC 616 (CanLII), 2020 CF 616 (CanLII). The judge \nfound that the Canadian Security Intelligence Service had breached its duty of candour. The duty of candour \nis a legal concept that applies in an ex parte warrant application and requires the party making the \napplication to demonstrate utmost good faith in presenting its case for a warrant. \n39 \nSamuelson-Glushko Canadian Internet Policy and Public Interest Clinic (CIPPIC), Facial Recognition at a \nCrossroads: Transformation at our Borders & Beyond, September 2020. \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n29 \nThe last one is that it often provides a link between digital and physical presences in \nways that allow for automation in the application of many other automated assessment \ntools, which is problematic in and of itself. \nMr. Israel made the following comment about the emergence of FRT at the borders: \nThere are proposals around the world to automate that screening process. You’ll walk \nup to a screen and get a facial recognition scan. There will be an assessment of your \nprofile pulled in digitally, and you’ll automatically get channelled through gates to a \nhigh-security, medium-security or low-security line. \nMr. Israel said that there are different types of FR systems, but that a decentralized \nsystem best describes the technology used for passport control. In a centralized system, \nall the images are held in one spot, whereas in a decentralized system an encoded digital \nimage, such as a passport photo, is compared to the photo taken by the passport user at \nthe airport. The security of the digital radio device encoded on the passport can be \nbreached, but then only one passport is breached. \nSeveral witnesses said that the use of FRT and other biometric technologies at airports \nand borders present high risks.40 \nEsha Bhandari, deputy director of the American Civil Liberties Union (ACLU), expressed \nconcern about the expansion of FR and other biometric technology in airports. She \nexplained that the concern is the mandatory requirement to agree to the use of FRT or \nprovide iris scans to access essential services such as going to the airport or crossing the \nborder, in contexts in which it is difficult to opt out due to the coercive nature of the \nenvironment. In her view, regulations should provide people with a meaningful opt-out \nso that they are not forced to provide an iris scan, for example.41 \nMr. Israel raised the lack of explicit opt-in, saying that “you’re not even necessarily aware \nthat you’re being subjected to the technology.” He gave the example of the customs \nscreening mechanisms at Toronto’s Pearson Airport where travellers do not necessarily \nrealize that a FR scan is happening. He said that “requiring, at the very least, an opt-out \nwith very clear notification, and perhaps even an opt-in, would be a useful addition.” \n \n40 \nETHI, Evidence, Tamir Israel; ETHI, Evidence, Esha Bhandari; ETHI, Evidence, Petra Molnar; Refugee Law Lab, \nBrief to the ETHI Committee – Study on the Use and Impact of Facial Recognition Technology, 25 April 2022 \n[Refugee Law Lab Brief]. \n41 \nRefugee Law Lab, p. 8. Iris scans are also used in some refugee camps, for example in Jordan. \n \n30 \nWitnesses also raised concerns about discrimination and racial profiling at the borders.42 \nFor example, Mr. Israel said that no-fly lists are a long-standing problem and that \nproposals to create FR lists with comparable objectives would be problematic. \nDr. Molnar again pointed out that FRT is highly discriminatory against black and brown \nfaces and that algorithmic decision-making often relies on biased datasets. This is what \nconcerns Prof. Jenkins about the use of FRT at airports, where even a 1% margin of error \ncan be massive. He explained: \nI think around 100,000 passengers per day travel through Heathrow Airport, so, if we \nhad an accuracy of 99% in that context, we’d be talking about 100 misidentifications per \nday, which soon adds up. It just doesn’t seem sustainable to me. \nFurthermore, in his research, Prof. Jenkins found that border and law enforcement \nofficials are no better at identifying unfamiliar faces, despite their professional training \nand many years of experience. He said that the percentage of human errors made by \nexperts, such as passport officers, compared to FRT software errors can vary depending \non the specifics of the task. For example, passport staff who are well trained and have \nmany years of experience have error rates of about 10%. For computer-based systems, it \nis difficult to predict accurate error rates since the results reported by vendors are often \nbased on ideal conditions that allow for reliable analysis, where real-world noise and \ncomplexity are not taken into account. \nDr. Molnar also shared her concerns about the use of FRT by border authorities for the \npurpose of implementing biometric mass surveillance in migration and border \nmanagement. In her view, to fully understand the impacts of various migration \nmanagement and border technologies (e.g., AI lie detectors, biometric mass surveillance \nand various automated decision-making tools), it is important to consider the broader \necosystem in which these technologies develop. It is an ecosystem that is increasingly \nreplete with the criminalization of migration, anti-migrant sentiments, and border \npractices leading to thousands of deaths, not only in Europe but also at the U.S.–Mexico \nand U.S.–Canada borders. \nSince 2018, Dr. Molnar has visited borders all around the world, most recently the U.S.–\nMexico border and the Ukrainian border. She said: \nBorders easily become testing grounds for new technologies, because migration and \nborder enforcement already make up an opaque and discretionary decision-making \nspace, one where life-changing decisions are rendered by decision-makers with little \n \n42 \nETHI, Evidence, Mustafa Farooq; ETHI, Evidence, Tamir Israel. \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n31 \noversight and accountability in a system of vast power differentials between those \naffected by technology and those wielding it.43 \nIn refugee determinations in particular, she said that, if mistakes are made, and if \nsomeone is wrongly deported to a country they are fleeing from, the ramifications can \nbe dire.44 Moreover, Dr. Molnar said that surveillance and smart border technologies do \nnot deter people from making dangerous crossings, but rather force them to change \ntheir routes towards less inhabited terrain, leading to loss of life. She gave the example \nof the family that was found dead at the Manitoba–U.S. border. \nDr. Molnar therefore believes that replacing human decision-makers with automated \ndecision-making and increasing surveillance “just muddies the already very discretionary \nspace of immigration and refugee processing and decision-making.” She added that, in \nborder enforcement and immigration decision-making, structures that are underpinned \nby intersecting systemic racism and historical discrimination against people migrating, \ntechnology’s impacts on people’s human rights are very real. \nWitnesses also discussed some specific FRT programs or projects at borders. \nFor example, Ms. Bhandari said the ACLU is concerned about the expansion of FR in \nairports, including programs like Nexus. Mr. Israel said that, while FRT programs used at \nCanadian borders, like Nexus, are still voluntary, the pressure to get through the border \nis used to encourage travellers to sign up for these types of systems. He also gave the \nexample of the World Economic Forum’s Known Traveller Digital Identity program pilot \nproject: \nCanada, for example, piloted a program with the Netherlands, one developed by the \nWorld Economic Forum. It’s basically a digital identity, housed on your phone, with a lot \nof your passport information and additional social identity verification program \ninformation. The idea was to see if that could be used in replacement of a passport, in \norder to facilitate border crossings. Facial recognition was the technology built into that \nsystem. The end vision of that system—it’s very explicit—is getting travellers to \nvoluntarily sign up for it to avoid delays at the border, because it gives you access to \nfaster security processing. However, it later becomes available to banks, \n \n43 \nDr. Molnar gave the example of what she saw in the Sonoran Desert at the U.S.–Mexico border where \nvarious automated and AI-powered surveillance towers are sweeping the desert. \n44 \nDr. Molnar referred to the following report: The Citizen Lab, Petra Molnar and Lex Gill, Bots at the Gate: A \nHuman Rights Analysis of Automated Decision-Making in Canada’s Immigration and Refugee System, \n26 September 2018. \n \n32 \ntelecommunication companies and other entities, as well, for similar identity \nverification programs.45 \nMr. Israel expressed concern about the Government of Canada’s participation in the \npilot project, which was interrupted by the pandemic. He said: \nI’m very concerned with the idea of using the pinpoint of the travel experience to \nencourage people to opt in and create these types of profiles, knowing that they're then \ngoing to be used against them, not just in border control contexts, where many \nmarginalized communities are already at a massive disadvantage, but here and abroad, \nin other countries that end up implementing the same system. It’s intended to be a \nglobal system. It’s also with the idea that these same systems are going to then be used \nby the private sector for fraud detection or identity management in interactions with \nprivate companies. \nDr. Molnar and Mr. Farooq also mentioned the CBSA’s pilot project at airports to test a \ntechnology called AVATAR—polygraphs that use facial and emotional recognition \ntechnologies to discern whether a person is lying and that are already banned in other \njurisdictions. While Dr. Molnar questioned how a detector can deal with religious or \nethnic differences, such as someone who may be reticent to make eye contact, who may \njust be nervous or who may have memory trauma, the NCCM expressed concerns about \nhow this technology can be weaponized to profile people for potential terrorism. \nDr. Molnar raised the following question: \nWhose priorities really matter when we choose to create AI-powered lie detectors at \nthe border instead of using AI to identify racist border guards? \nLastly, Mr. Israel said that recently “CBSA announced they will try to implement a \nbiometric study hub within their infrastructure,” but that not much has been seen going \non yet. \nUse of Facial Recognition in Public Spaces \nMs. Khoo said that the use of FRT in public violates privacy preserved through anonymity \nin daily life. She said that this would likely induce chilling effects on freedom of \nexpression such as public protests about injustice. It also promises to exacerbate gender-\nbased violence and abuse by facilitating the stalking of women. Witnesses also reminded \n \n45 \nETHI, Evidence, Tamir Israel; Government of Canada, The Government of Canada to test cutting-edge \ntechnologies to support secure and seamless global travel for air passengers, News release, \n25 January 2018. \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n33 \nthe Committee that the Supreme Court of Canada ruled that individuals retain a right to \nprivacy even when in a public space.46 \nDr. Watkins said that the use of FRT in public spaces could affect the right to freedom \nof movement. \nMr. McSorley said that, even if the significant problems of bias and accuracy were \nresolved, FR surveillance systems would continue to subject people to intrusive and \nindiscriminate surveillance, whether it is people walking through a public square or \nactivists at a protest. \nMr. Farooq said his organization has not received any formal human rights complaints \nrelated to AI technology, including FRT. He mentioned, however, that the NCCM \nsometimes hears concerns around people attending peaceful rallies, such as in \nVancouver or Hamilton, where pictures are being taken by law enforcement. He \nspecified that the NCCM does not necessarily know what is being done with the \ncollected data, in large part because of lack of disclosure. In his view, the lack of \ncomplaints is due to a lack of disclosure. \nFRT is also used in semi-public spaces, such as commercial establishments. Mr. Labonté \ndrew a parallel between using FRT in a retail store or shopping centre with e-commerce, \nwhere even if users are not connected to an account, cookies nevertheless leave behind \ntraces of that time on the web. These cookies are then used to send targeted advertising \non the basis of preferences. \nMs. Bhandari gave the example of U.S. companies, such as Walgreens, that use FRT to \npick out a customer’s age and gender and show them tailored ads or products. She said \nthat this is an invasive tactic that could lead to concerns about consumers being steered \nto products based on gender stereotypes, which could further segregate society. \nMs. McPhail mentioned the OPC’s investigation of Cadillac Fairview Mall. She said that \nthe investigation revealed a non-consensual private sector use of facial analytics, which \nwas discovered due to a glitch in the technology. In her view, this example shows that \n“almost every facial recognition vendor advertises that it can help private sector bodies \nleverage personal data to improve their market, and that’s a problem.” \n \n46 \nICLMG Brief, p. 6; Ligue des droits et libertés Brief, p. 4; R v. Spencer, 2014 SCC 43. \n \n34 \nIn its investigation, the OPC found that Cadillac Fairview had collected and used personal \ninformation, including sensitive biometric information through anonymous video \nanalytics, without valid consent from visitors to Canadian malls.47 \nThe examples given by witnesses show that FRT surveillance can be conducted in public \nspaces without people’s knowledge. \nUse of Facial Recognition in the Workplace \nDr. Watkins expressed her concerns with the private industry use of facial verification on \nworkers. She said that “[f]acial verification is increasingly being used in work contexts, in \nparticular gig work or precarious labour.” \nAccording to Dr. Watkins, “these systems are often in place to guarantee worker privacy, \nto prevent fraud and to protect security” but there needs to be alternatives in place to \ngive workers other options. She said that workers should be consulted to better \nunderstand what kinds of technology they would prefer to comply with and to provide \nthem with alternatives so that they can opt out of technologies yet still access their \nmeans of livelihood. She explained: \nIn my research, I’ve gathered data from workers describing a variety of harms. They’re \nworried about how long their faces are being stored, where they’re being stored and \nwith whom they’re being shared. In some cases, workers are forced to take photos of \nthemselves over and over again for the system to recognize them as a match. In other \ncases, they’re erroneously forbidden from logging into their account because the system \ncan’t match them. They have to spend time visiting customer service centres and then \nwait, sometimes hours, sometimes days, for human oversight to fix these errors. In \nother cases still, workers have described being forced to step out of their cars in dark \nparking lots and crouch in front of their headlights to get enough light for the system to \nsee them. When facial verification breaks, workers are the ones who have to create and \nmaintain the conditions for it to produce judgment. \nDr. Watkins said that ultimately FRT is currently not reliable enough to be used in high-\nrisk scenarios like the workplace. However, she acknowledged that some workers do \nadvocate for FR for various reasons. \n \n47 \nOPC, Joint investigation of the Cadillac Fairview Corporation Limited by the Privacy Commissioner of Canada, \nthe Information and Privacy Commissioner of Alberta, and the Information and Privacy Commissioner for \nBritish Columbia, 28 October 2020. \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n35 \nUse of Facial Recognition by Political Parties \nAccording to Ms. McPhail, the use of FR by political parties also poses a risk to \ndemocracy. She mentioned the Liberal Party of Canada’s recent use of a “similar one-to-\none matching facial recognition tool … in its nomination voting process prior to the last \nfederal election.” She said that “[i]n that case, it was a much more risky use of a \npotentially faulty and discriminatory technology because it took place in a process that is \nat the heart of grassroots democracy.” \nCommittee Observations and Recommendations \nThe Committee is of the view that RCMP officials were very reluctant to provide \ncomplete answers to the Committee’s questions. In particular, with respect to the use of \nClearview AI technology, many members voiced concern during the RCMP’s testimony \nthat witnesses were being evasive in their responses. \nConsidering the Privacy Commissioner’s comments on section 4 of the Privacy Act and \nthe many concerns about the use of FRT in the various contexts explored in this chapter, \nthe Committee recommends: \nRecommendation 1 \nThat the Government of Canada amend section 4 of the Privacy Act to require a \ngovernment institution to ensure that the practices of any third party from which it \nobtains personal information are lawful. \nRecommendation 2 \nThat the Government of Canada ensure that airports and industries publicly disclose the \nuse of facial recognition technology including with, but not limited to, signage \nprominently displayed in the observation area and on the travel.gc.ca website. \nRecommendation 3 \nThat the Government of Canada refer the use of facial recognition technology in military \nor intelligence operations, or when other uses of facial recognition technology by the \nstate have national security implications, to the National Security and Intelligence \nCommittee of Parliamentarians for study, review and recommendation; and that the \nCommittee report its findings. \n \n36 \nRecommendation 4 \nThat the government, in the creation of its regulatory framework around the use of facial \nrecognition technology, set out clear penalties for violations by police. \nCHAPTER 3: ACCOUNTABILITY, PROCUREMENT AND PUBLIC \nINVESTMENT \n“Very little money, time and resources go into dealing \nwith the mess these technologies create and the harm \nthey create.” \nAna Brandusescu, artificial intelligence governance expert, \nwho appeared as an individual on 21 March 2022. \nAccountability \nTransparency \nWith respect to transparency about how FRT works, Ms. Wang said that models that \nhave been trained using machine learning to perform FR tasks are currently difficult to \ninterpret since it is not clear what patterns the models are relying on. \nOn the other hand, explaining the model is not necessarily the solution. \nMs. Brandusescu said that, while explainable AI is a computational solution to make sure \nFRT can go forward, the explanation depends on the audience, and that audience is \nusually comprised of computer scientists, not politicians. She said trying to understand \nthe black box is important but that having an explanation does not mean the technology \nshould be used. \nMs. Piovesan also raised the importance of being able to explain the technology, \ni.e., understanding how algorithms operate and the output they provide and having \nindependent verification to ensure that the output is accurate and reliable. She also \nnoted the importance of having meaningful discussion with a variety of stakeholders \nabout how these technologies are used and their implications before they are rolled out. \nShe said one way to achieve this goal is to apply and adopt the concept of radical \ntransparency. \nMs. Piovesan explained that radical transparency speaks to the entire disclosure process. \nShe encourages organizations that use advanced technology to let people know who \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n37 \ntheir vendors are, what their uses are, where they are collecting the data they use and \nwhy they are doing so. Radical transparency seeks to engage the public rather than \nfoster a secretive environment that undermines people’s trust. \nProf. Jenkins also emphasized that transparency is important because the public needs \nto understand how these technologies are being used, how they can be effective, and \nhow they may affect them. Auditing the use of FRT and making it public, for example, \nwould help with transparency. He said that transparency is also an important \ncomponent of an ethical system. \nMr. McSorley said that there needs to be pressure to have greater transparency and \naccountability from government. He said that federal agencies are required to conduct \nprivacy impact assessments before new technology or private-impactful projects are \nundertaken but those assessments are often not done at all, or are kept secret.48 For \nexample, he said that the National Security and Intelligence Review Agency is \nundertaking a review of the use of biometric surveillance, but that it could take a couple \nof years before it is made public.49 He argued that “a lack of transparency and \naccountability means that such technology is being adopted without public knowledge, \nlet alone public debate or independent oversight.” \nMr. Farooq made complementary comments. He said that it is hard to engage with \ngovernment agencies when basic facts are not being acknowledged. For example, when \nCSIS refuses to confirm whether it uses FRT, it is hard to get any sense of accountability \nfrom it. \nMs. Khoo said that, in the case of police, policies governing the use of FR “can be even \nmore of a black box than the algorithms themselves are said to be.” She said this lack of \ntransparency gives rise to severe due process deficits in criminal cases. She \nrecommended that robust transparency and accountability measures be established. \nMs. Brandusescu suggested the Treasury Board should be involved in creating a registry \nfor AI, especially AI used for law enforcement and national security purposes. She said \nsuch a registry would be useful for researchers, academics and investigative journalists \nwho inform the public. \n \n48 \nThe Government of Canada’s Directive on Privacy Impact Assessment has been in effect since 2010. It \napplies to government institutions subject to section 3 of the Privacy Act. \n49 \nNational Security and Intelligence Review Agency, National Security and Intelligence Review Agency - 2022–\n23 Departmental Plan. The departmental plan states that the Agency is conducting an ongoing review of the \nuse of biometrics. \n \n38 \nDr. Watkins said better insight is needed into how technology tools like FRT are being \nused, where the data is being stored, how decisions are being made with them, and \nwhether or not humans are involved. In short, more transparency is needed. \nGovernance and Accountability \nDr. LaPlante said that, since biometric data is sensitive, the security of that data must \nbe ensured when it is collected, used and stored. According to her, FRT, like any high-risk \nAI system, should undergo extensive validation so that its limitations are properly \nunderstood and taken into consideration when applied in the real world. With respect to \nAI governance, Dr. LaPlante said: \n[G]overnance requirements should be proportional to risk materiality. Impact \nassessments should be common practice, and there should be context-dependent \noversight on issues of technical robustness and safety, privacy and data governance, \nnon-discrimination, and fairness and accountability. This oversight should not end once \na system is in production but should instead continue for the lifetime of the system, \nrequiring regular performance monitoring, testing and validation. \nTo root out bias from technology, Dr. LaPlante recommended that the Committee look \ninto the concept of ethics by design, which involves taking ethical considerations into \naccount throughout the development cycle, from initial data collection, to algorithm \ndevelopment, to production, and to the monitoring of those systems. \nMr. Labonté explained that, when a system is biased, it means that the initial data \nsamples are not equal or are not representative in an equal way. He said that it is \nessential to regulate data harvesting, after noting that the most competitive players at \nthe moment are the ones that collected enormous amounts of data for use in training AI \nmodels. \nMr. Maslej said that, in some cases, the more data provided to an AI model, the more \nlikely it is to contain biased data. If data is not proactively filtered, AI models are likely to \nbehave in problematic ways. He explained that filtering the data used to train an AI \nmodel could fix the problem, but would likely affect the model’s ability to perform \noptimally. \nMs. Wang said that to correct bias problems in FRT results, providers should collect \nmore diverse and inclusive data sets, and perform disaggregated analyses to look at the \naccuracy rates across different demographic groups rather than looking at one overall \naccuracy metric. She noted, however, that the collection of such data sets may itself be \nexploitative of marginalized groups by violating their privacy. \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n39 \nMs. Brandusescu also suggested prioritizing accountability. For example, in the public \nsector, she said that the RCMP should be required to publish a report explaining its use \nof FRT. That practice could be applied to all federal departments and agencies in the \nfuture.50 \nFederal institutions are already required to comply with the Treasury Board’s Directive \non Automated Decision-Making. The federal directive requires, among other things, that \nan algorithmic impact assessment (AIA) be completed prior to the production of any \nautomated decision system. Depending on the impact level, different accountability \nmechanisms are required (e.g., peer review or human involvement). The directive also \nimposes transparency and quality assurance obligations, such as making the AIA public. \nHowever, Ms. Brandusescu said that the federal directive needs to be improved. She \nbelieves the public should have information about the government’s use of technology \nlike FRT and get updates. For example, she suggested that the Treasury Board publish \nrecent government involvement in AI on its website (e.g., the procurement of new \ntechnologies such as FRT). She also recommended more specific, ongoing monitoring \nrequirements for AI systems after the initial AIA, such as if the use or impact of the \nsystem changes. \nMs. Brandusescu also noted that the only non-governmental stakeholders consulted \nin AIAs published by the government since the federal directive came into effect were \ncompanies.51 She suggested that AIAs could be improved by engaging civil society. She \nargued that engaging only companies limits the input of Canadians, affected groups, \ndigital rights organizations and civil society bodies. \n \n50 \nMs. Brandusescu recommended that the Privacy Commissioner demand this report. The Commissioner \ndoes not currently have order-making powers. Bill C-27, An Act to enact the Consumer Privacy Protection \nAct, the Personal Information and Data Protection Tribunal Act and the Artificial Intelligence and Data Act \nand to make consequential and related amendments to other Acts, introduced in the House of Commons in \nJune 2022, if passed in its current form, would give the Commissioner the power to issue orders for the \nenforcement of federal privacy legislation that applies to the private sector. The Privacy Act applies to the \npublic sector, including to the RCMP. \n51 \nThe Directive on Automated Decision-Making requires that, depending on the level of impact of the \nautomated decision system, peer review be conducted by one of the following groups or means: a qualified \nexpert from a federal, provincial, territorial or municipal government institution; qualified members of \nfaculty of a post-secondary institution; qualified researchers from a relevant non-governmental \norganization; a contracted third-party vendor with a related specialization; publishing specifications of the \nautomated decision system in a peer-reviewed journal; a data and automation advisory board specified by \nTreasury Board Secretariat. \n \n40 \nDr. Watkins said there is a need to ensure accountability and build the kinds of \nrelationships between government, private actors and the public interest to address the \nneeds of the most vulnerable. \nMs. Brandusescu also noted the need to increase in-house AI expertise so that agencies \nunderstand the technology they are buying. \nIn the same vein, Prof. Jenkins recommended “attention to human operators in the \ndesign and implementation of facial recognition systems, transparency and the \ndevelopment of an expert workforce in facial recognition.” Though he added: “Human \noversight provides important safeguards and a mechanism for accountability; however, it \nalso imposes an upper limit on the accuracy that face recognition systems could achieve \nin principle.” In other words, whenever there is human oversight, there is a risk of \nhuman error. To mitigate this risk, it is important to ensure that the people involved in FR \ndecisions are highly qualified.52 \nProcurement and Public-Private Partnerships \nMs. Brandusescu said that the issue relating to technologies like FRT is more than data \nprotection or privacy; it is a conversation about private sector involvement in public \ngovernance. She said people should be very concerned about the private sector taking \nover government policy development to regulate AI and FRT. Public-private partnerships \nare a key element of procuring, deploying, developing and using these technologies. She \nsaid that, in a recent report, she and her colleague argued that taxpayers are essentially \npaying to be surveilled, while companies like Clearview AI can exploit public sector \ntechnology procurement processes and the lack of regulatory mechanisms.53 \nAs an example of perceived flaws in the procurement process, Ms. Brandusescu raised \nthe fact that Palantir Technologies Inc. is on the federal government’s list of pre-qualified \nAI suppliers, despite reports that the company has committed human rights violations in \nthe U.S. and elsewhere (e.g., in immigration, mass arrests and the separation of children \nfrom their parents).54 She said companies linked to human rights abuses should be \nremoved from the government’s list. \n \n52 \nETHI, Evidence, Rob Jenkins. \n53 \nCentre for Media, Technology and Democracy, Yuan Stevens and Ana Brandusescu, Weak privacy, weak \nprocurement: The state of facial recognition in Canada, 6 April 2021. \n54 \nGovernment of Canada, Treasury Board Secretariat, List of interested Artificial Intelligence (AI) suppliers; \nAmnesty International, Failing to do Right: The Urgent Need for Palantir to Respect Human Rights, 2020; \nETHI, Evidence, Ana Brandusescu. \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n41 \nMoreover, Ms. Brandusescu said that AI can sometimes evade procurement policies by \noffering free software trials, as was the case with Clearview AI.55 She said that, to \nimprove public procurement, a policy for the proactive disclosure of free software trials \nused by law enforcement and all of government should be created, as well as a public \nregistry for them. This would “make the black box a glass box.” \nDr. Molnar urged the Committee to consider why the private sector often gets to \ndetermine what Canada innovates on and why through public-private partnerships, \nwhich states are increasingly keen to make in today’s global AI arms race. She reiterated \nthe need to “pay careful attention to the particular actors involved in the ecosystem in \nwhich these technologies develop and are deployed.” According to her: “None of this is \nneutral. It is all a political exercise.” \nMs. Khoo and Prof. Thomasen raised the emergence of Amazon Ring-police partnerships \nin the U.S. as an example of surveillance infrastructure using a public-private \npartnership. \nMr. McSorley noted that, without proper regulation and with so many companies \nproposing their technology to law enforcement agencies, it is hard to know whether \nthey “will even be using the most accurate [technology]—or will they be using the most \naccessible, the ones that are targeted more and marketed more towards law \nenforcement”? He too believes that the lack of regulation is what allowed the RCMP to \nuse Clearview AI’s FRT for months without the public’s knowledge. \nNeither Mr. Israel nor Ms. Bhandari were aware of a centralized registry of companies \noffering FRT, but said that some U.S. states require data brokers to register. Ms. Bhandari \nsaid requiring that kind of transparency from companies selling FR or other algorithmic \ntools would allow for a private right of action or for regulators to know who they should \nbe monitoring. \nMs. Thomasen suggested that in-house FR systems be developed using data that is \nlegally sourced, with informed consent and through processes that ensure the dignity of \nthe individuals whose data is being processed. These systems could be designed and \nused only for very specific use cases, as opposed to commercial systems that do not take \ninto account the specific social context in which FRT is used. \n \n55 \nSpecial Report on the RCMP, para. 1. The OPC reported that the RCMP confirmed that it purchased two \nlicenses to use Clearview AI’s services in 2019 and that RCMP members had also used Clearview AI \ntechnology via free trial accounts. \n \n42 \nProcurement by Police Forces \nWith regard to police procurement of technology, Ms. Khoo explained that that strict \nlegal safeguards must be in place to ensure that police reliance on private sector \ncompanies does not create a way to go around people’s rights to liberty and protection \nfrom unreasonable search and seizure. \nFor example, software from companies like Clearview AI, Amazon Rekognition and NEC \nCorporation is typically protected by trade secret laws and procured on the basis of \nbehind-the-scenes lobbying. Ms. Khoo says this circumstance results in secretive public-\nprivate surveillance partnerships that strip defendants of their due process rights and \nsubject the public to inscrutable layers of mass surveillance. To address this situation, \nshe recommended that any commercial technology vendor that collects personal data \nfor law enforcement should be contractually bound or otherwise held to standards of \nprivacy and disclosure. \nMs. Khoo made three specific recommendations about the law enforcement \nprocurement process to protect privacy and ensure accountability: \n• law enforcement’s acquisition of FRT or algorithmic policing technology \ncan be done without engaging with a commercial vendor so as not to be \nbeholden to proprietary interests (e.g., developing FRT in-house);56 \n• if procurement must be from a commercial vendor, strict procurement \nconditions can be put in place (e.g., waiving trade secrets for \nindependent auditing); and \n• ensure less secrecy around contracts so that people know about them \nprior to being signed rather than through leaks, freedom of information \nrequests, or investigations by journalists. \nMs. Piovesan agreed with Ms. Khoo’s recommendations. \nPublic Investment \nMs. Khoo said that private companies that collect vast quantities of data to capitalize on \nit are often funded through government grants, whether through the guise of innovation \n \n56 \nMs. Khoo gave the example of a lab in Saskatchewan (the Saskatchewan Police Predictive Analytics Lab), a \npublicly funded collaboration between the municipal police force and the University of Saskatchewan, \nwhich built their FRT in-house. \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n43 \nor because of lobbying. She said this is “essentially government and private companies \nworking hand in hand to build out this network of surveillance.” \nMs. Brandusescu said there is a bigger question about Canada’s military-industrial \ncomplex and where surveillance technologies like FRT come from. She believes Canada \nneeds to reflect on what tech solutionism means and why so much money is put into \ntech innovation but not into funding groups who work hard on social issues to \nunderstand the technology and create public awareness and education about it. \nMs. Brandusescu believes that the government should not fund FRT. Instead, it should \nfund civil society, digital rights groups and community groups that are studying FRT and \ninvolve them in the conversation about what the government decides to fund. She said: \nI think we can push back on tech inevitability, and we can say no to some of this \ntechnology, but that also requires funding and resources for education around these \ntechnologies. A lot of these contracts are made behind closed doors. In industry-\ngovernment relationships, the public-private partnerships sometimes involve \nuniversities and labs, but it’s always for a private interest focus. You want to fund these \ntechnologies, to build them, and then to use them. You don’t think about the \nconsequences. Very little money, time and resources go into dealing with the mess \nthese technologies create and the harm they create. \nMs. Polsky said that Canadians are not fully aware of their privacy rights. She raised the \nidea of developing education programs for schools, but acknowledged that education is \na provincial jurisdiction. She said that some media organizations and privacy \ncommissioners across the country have developed courses or programs, but that these \nare not mandatory. She suggested that the OPC should be given an education mandate \nand funding for awareness campaigns.57 \nExample of Accountability in Action: Microsoft \nMicrosoft shared some of its internal AI and FRT practices, portraying itself as an \nexample of a responsible AI provider in the private sector. \nMr. Larter said that Microsoft has a broad responsible AI program with three main \ncomponents: a company-wide AI governance team that includes multiple stakeholders, \nincluding world-leading researchers; an AI standard, which ensures that any teams that \n \n57 \nThe federal Privacy Commissioner already has an educational mandate under section 24 of the Personal \nInformation Protection and Electronic Documents Act, which requires the Commissioner to “develop and \nconduct information programs to foster public understanding, and recognition of the purposes, of this \nPart.” This obligation remains in the proposed Consumer Privacy Protection Act in Bill C-27. A similar \nprovision is not found in the Privacy Act. \n \n44 \nare developing or deploying AI systems are doing so in a way that meets AI principles; \nand a sensitive use review process.58 \nMr. Larter explained that a sensitive use review is done when any potential development \nor deployment of an AI system hits one of three triggers: the system is used in a way that \naffects a person’s legal opportunities or legal standing; there is a potential for \npsychological or physical harm; or there is an implication for human rights. In such cases, \nMicrosoft’s governance team meets and reviews whether the company can move \nforward with a particular AI system.59 \nWith respect to FRT, Mr. Larter said Microsoft published a transparency note for its Face \napplication programming interface (API). The note explains in plain language how FRT \nworks, what its capabilities and limitations are, and factors that affect performance. He \nsaid that, as Microsoft was developing its FRT, it was very mindful of having \nrepresentative datasets so that the technology can perform accurately, including across \ndifferent demographic groups. \nMr. Larter said testing is really important given the wide gap between the best \nperforming facial recognition systems and the least well-performing systems. He said \nvendors should allow their systems to be tested by independent third parties in a \nreasonable fashion and be required to address any material performance gaps.60 \nMicrosoft allows its systems to be tested. He also said there is a need for robust \ncybersecurity around technology. \nCommittee Observations and Recommendations \nThe Committee believes that the government should be more transparent about its use \nof FRT and other AI, as well as about its procurement process. It should also invest more \nin studying the impact of AI and raising awareness about privacy rights. \nTherefore, the Committee recommends: \n \n58 \nIbid. Microsoft, Putting principles into practice at Microsoft. Microsoft’s AI standard consists of a series of \nrequirements related to its six AI principles: fairness, reliability and safety, privacy and security, \ninclusiveness, transparency and accountability. \n59 \nIbid. \n60 \nETHI, Evidence, Owen Larter. According to Mr. Larter, there should be a testing requirement for \norganizations deploying FRT to make sure that the system is working accurately in the environment in which \nit’s going to be used. \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n45 \nRecommendation 5 \nThat the Government of Canada amend its procurement policies to require government \ninstitutions that acquire facial recognition technology or other algorithmic tools, \nincluding free trials, to make that acquisition public, subject to national security \nconcerns. \nRecommendation 6 \nThat the Government of Canada create a public AI registry in which all algorithmic tools \nused by any entity operating in Canada are listed, subject to national security concerns. \nRecommendation 7 \nThat the Government of Canada enhance the Treasury Board Directive on Automated \nDecision-Making to ensure the participation of civil society groups in algorithmic impact \nassessments and to impose more specific requirements for the ongoing monitoring of \nartificial intelligence systems. \nRecommendation 8 \nThat the Government of Canada increase its investment in initiatives to study the impact \nof artificial intelligence on various demographic groups, increase digital literacy, and \neducate Canadians about their privacy rights. \nRecommendation 9 \nThat the Government of Canada ensure the full and transparent disclosure of racial, age \nor other unconscious biases that may exist in facial recognition technology used by the \ngovernment, as soon as the bias is found in the context of testing scenarios or live \napplications of the technology, subject to national security concerns. \nRecommendation 10 \nThat the Government of Canada establish robust policy measures within the public \nsector for the use of facial recognition technology which could include immediate and \nadvance public notice and public comment, consultation with marginalized groups and \nindependent oversight mechanisms. \n \n46 \nCHAPTER 4: REGULATING FACIAL RECOGNITION TECHNOLOGY AND \nARTIFICIAL INTELLIGENCE \n“FRTs need to be regulated with a scalpel, not an axe.” \nCarole Piovesan, managing partner with INQ Law,  \nwho appeared before the Committee on 21 March 2022 \nAs Ms. Polsky pointed out, the Supreme Court of Canada recognized long ago that \n“privacy is essential for the well-being of the individual” and that “[g]rounded in man’s \nphysical and moral autonomy, privacy is essential for the well-being of the individual.”61 \nHowever, given the place of AI and FRT already in our society, the Committee wondered: \nIs it too late to intervene? \nMost witnesses said it was not too late.62 Others said that the proliferation of FRT does \nnot spell the end of individual freedom.63 Mr. Therrien also felt that it was not too late to \nintervene. He explained: \nI heard you ask certain witnesses at this committee if it’s too late. It’s never too late. \nActually, the fact that certain practices are currently occurring should be no reason for \nyou to prevent yourself from doing the right thing and regulating the technology in a \nway that respects the rights of Canadians. \nWe are living, not completely but in part, in a world of self-regulation that has led to \ncertain unacceptable practices. It’s not because they are routine or banal … that they \nshould continue to be authorized. \nWitnesses suggested a number of ways to address shortcomings in the current \nlegislative regime. \nMoratoriums, Bans and Other Measures \nGiven the risks of FRT, most stakeholders recommended a moratorium, particularly \nin law enforcement, until an appropriate regulatory framework is in place and more \n \n61 \nR v. Dyment, [1988] 2 SCR 417, para. 17 (Justice La Forest). \n62 \nETHI, Evidence, Cynthia Khoo; ETHI, Evidence, Carole Piovesan; ETHI, Evidence, Rob Jenkins; ETHI, Evidence, \nDaniel Therrien; ETHI, Evidence, Esha Bhandari; ETHI, Evidence, Tamir Israel. \n63 \nETHI, Evidence, Cynthia Khoo; ETHI, Evidence, Ana Brandusescu; ETHI, Evidence, Sanjay Khanna; ETHI, \nEvidence, Rob Jenkins. \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n47 \nresearch and consultation on the use of the technology and its impacts have \nbeen done.64 \nFor example, Ms. Khoo said there should be a moratorium, or a national pause, on the \nuse of FRT by law enforcement until it is shown to be not only reliable but also necessary \nand proportionate to legitimate aims and the far-reaching repercussions its use may \nhave. She did not rule out a potential ban on the use of FRT in some cities, as is the case \nin the U.S.65 \nMs. Khoo said a moratorium on the use of FRT by law enforcement will give time for \nfurther research to determine whether it is appropriate to use it and with what \nsafeguards, such as transparency, adequate oversight mechanisms and disclosure \nrequirements. She called for a moratorium not only on FRT but on all algorithmic \npolicing technologies. She also proposed that, during the moratorium, a national \ncommission or judicial inquiry do an in-depth constitutional and human rights analysis to \ndetermine what is appropriate and what is not.66 \nMr. McSorley and Prof. Thomasen recommended that, during a moratorium, the federal \ngovernment hold consultations on the use and regulation of FRT, for example to decide \nwhat uses should be prohibited. \nMs. McPhail said that one purpose of a moratorium would be to give the government a \nchance to rectify a major gap in the federal privacy regime: the fact that the \nCommissioner does not have enforcement powers. She said that a moratorium for law \nenforcement is particularly important because those are situations where the \nconsequences of an error can be life-altering, but that a general moratorium would also \n \n64 \nETHI, Evidence, Cynthia Khoo; ETHI, Evidence, Kristen Thomasen; ETHI, Evidence, Brenda McPhail; ETHI, \nEvidence, Sanjay Khanna; ETHI, Evidence, Elizabeth Anne Watkins; ETHI, Evidence, Angelina Wang; ETHI, \nEvidence, Tim McSorley; ETHI, Evidence, Rizwan Mohammad; ETHI, Evidence, Mustafa Farooq; ETHI, \nEvidence, Sharon Polsky; ETHI, Evidence, Tamir Israel; CMTD and CPE Brief; ICLMG Brief; Tessono Brief; Ligue \ndes droits et libertés Brief; Canadian Human Rights Commission, Brief to the ETHI Committee – Study on the \nUse and Impact of Facial Recognition Technology, April 2022 [CHRC Brief]. \n65 \nETHI, Evidence, Cynthia Khoo; ETHI, Evidence, Esha Bhandari. According to Ms. Bhandari, at least 23 cities in \nthe U.S. have halted law enforcement or government use of facial recognition technology. See also: CMTD \nand CPE Brief, p. 8; Tessono Brief, p. 5. \n66 \nETHI, Evidence, Cynthia Khoo. The Citizen Lab report, co-authored by Ms. Khoo, defines algorithmic policing \nas all new technologies that use an automated mathematical formula to support or supplement police \ndecision-making. \n \n48 \nbe beneficial given that private sector vendors are selling technologies to public sector \nactors.67 Ms. Khoo also said that a moratorium in other sectors would be appropriate. \nSome witnesses were against the idea of a moratorium. For example, Mr. Larter said that \nMicrosoft believes that, instead of investing time and effort in imposing a moratorium, \npolice use of FRT should be regulated. He said that Microsoft supports FRT regulation \nthat protects human rights, prohibits mass surveillance and advances transparency and \naccountability. In his view, a regulatory framework would build public trust in the use \nof FRT. \nMr. Larter noted, however, that Microsoft has imposed a moratorium on the sale of \nits FRT to police forces in the U.S. He said that this ban does not apply to Canada, \nbecause unlike Canada, the U.S. does not have a federal privacy framework or broad \nprivacy laws.68 \nMr. Stairs with the TPS does not believe a moratorium on the use of FRT by police forces \nshould be imposed until the technology is further regulated. In his view, there is a \nbalance between the public security and safety benefits of FRT and the human rights \nchallenges with the technology. The important thing is to know when to deploy the \ntechnology. For the TPS, it is only in major crimes and major cases. \nMr. Therrien also said he was not in favour of a complete moratorium. He said that a \nmoratorium can be imposed by legislation only. The Commissioner does not have the \npower to impose a moratorium. The position of the federal, provincial and territorial \nprivacy commissioners is that legislation should prescribe when FR can be used for \nlegitimate, helpful purposes and social good (e.g., investigating serious crimes or finding \nmissing children). The legitimate uses should be defined narrowly, and the law should \nalso prescribe prohibited uses. That ban would be a partial moratorium on the use \nof FRT. \nMr. Therrien said he believes in the use of FRT in compelling circumstances. He said that, \nif the RCMP were to use FRT only according to its new policy (targeted, time-limited use, \nsubject to verification by trained experts, and as an investigational aid, not to confirm an \n \n67 \nETHI, Evidence, Brenda McPhail. Ms. McPhail gave the example of a U.S. bill to place a moratorium on \ngovernment use of facial recognition technology until rules governing its use are in place; Congress.gov., \n“Text - S.3284 - 116th Congress (2019-2020): Ethical Use of Facial Recognition Act,” 12 February 2020. The \nbill was referred to a U.S. Senate committee, but has not made further progress. \n68 \nETHI, Evidence, Owen Larter, 1550 and 1610. \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n49 \nidentity) that would also be a form of voluntary partial moratorium until the legislation is \nimproved.69 \nWith respect to bans, Ms. McPhail said that CCLA supports a complete ban on mass \nsurveillance uses of FRT.70 Dr. Molnar said that there are ongoing discussions in Europe \nabout an outright ban on biometric mass surveillance, high-risk FRT and AI lie detectors \nin migration and border management. She said Canada should ban the high-risk use of \nFRT at the border. Mr. Farooq said that the use of real-time FRT at airports and borders \nshould be banned. Ms. Bhandari recommended banning government and law \nenforcement use of FRT. Mr. Israel recommended a permanent ban on the use of \nautomated, live biometric recognition by police in public spaces. \nIn addition to a moratorium or ban, Mr. Khanna suggested adopting a digital charter of \nrights for Canadians that would recognize the sanctity of personal data, like facial data. \nSuch a charter could align with the Canadian Charter of Rights and Freedoms. He said it \nwould allow Canadians to own and have a portable and secure form of biometric data \nthat is considered sacrosanct. He also encouraged legislators to use scenario planning to \ninform resilient strategy and public policy in the face of digital advances. \nPrivacy Guidance on Facial Recognition for Police Agencies \nOn 2 May 2022, the federal privacy commissioner and his provincial and territorial \ncounterparts issued guidance on facial recognition for police agencies.71 \nMr. Therrien explained that the guidance is meant to assist police in ensuring that any \nuse of FRT complies with the law, minimizes privacy risks and respects privacy rights. He \nsaid that the guidance was developed following a national public consultation with a \nbroad range of people. Stakeholders agreed that current laws were inadequate. \nHowever, there was no consensus on the content of a new law.72 \n \n69 \nETHI, Evidence, Daniel Therrien, 1135 and 1210. \n70 \nSee also: Ligue des droits et libertés Brief, p. 8. The organization believes that three uses of FRT should be \nimmediately prohibited through legislation: mass surveillance of public places; mass online surveillance; and \nuse of image banks created by public agencies or departments. \n71 \nOPC, Privacy guidance on facial recognition for police agencies, May 2022. \n72 \nStakeholders representing civil society, minority groups and the police itself participated in the consultation. \nMr. Therrien met a number of times with the Royal Canadian Mounted Police and the Canadian Association \nof Chiefs of Police. His colleagues also met with provincial equivalents. \n \n50 \nMr. Therrien noted that, until the laws are amended, the guidance offers advice to police \non how to use FRT under current laws. He hopes that the guidance will mitigate risks. \nMr. Therrien also acknowledged that some stakeholders wanted the guidance provided \nby the commissioners to include advice on use cases. While he agreed on the need for \nadvice on particular uses in different contexts, he said the commissioners thought it \nimportant and relevant to have general guidance that can be augmented as use cases \nare developed. \nMs. Kosseim said it may take several years before there is any jurisprudence on FRT \nunder the Charter. It is why the commissioners recommend the adoption of a legislative \nframework. In the interim, they developed the guidance to help mitigate risks. \nMs. Kosseim presented the five key elements of the guidance: \nFirst, before using facial recognition for any purpose, police agencies must establish that \nthey are lawfully authorized to do so. This is not a given, and cannot be assumed … \nSecond, police agencies must establish strong accountability measures. This includes \ndesigning for privacy at every stage of a facial recognition initiative and conducting a \nprivacy impact assessment, or PIA, to assess and mitigate risks in advance of \nimplementation … \nThird, police agencies must ensure the quality and accuracy of personal information \nused as part of a facial recognition system to avoid false positives, reduce potential bias \nand prevent harms to individuals and groups … \nFourth, police agencies should not retain personal information for longer than necessary \n… \nFifth, policy agencies must address transparency and public engagement. Direct notice \nabout the use of facial recognition may not always be possible in the context of a \nspecific police investigation. However, transparency at the program level is certainly \npossible. \nMs. Kosseim also made clear that any communication with the public should be two-\nway. She said that key stakeholders, particularly representatives of over-policed groups, \nshould be consulted in the very design of a police service’s FR program. She added that, \ngiven the importance of reconciliation in Canada, this must include input from \nIndigenous groups and communities. \nMs. Kosseim said that the basic principles advanced in the guidance should apply \nregardless of the sector, but with the necessary adjustments for other contexts and the \nrange of risks at play, since it was specifically designed for police services. \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n51 \nMr. Therrien made similar comments, noting that the common factor that applies \nhorizontally to all stakeholders who would like to use FRT, whether police services, \nbusinesses or government, is the principle of necessity and proportionality. For example, \nin police services, the use of FRT can have extremely serious consequences, resulting \neven in the loss of freedom. A total prohibition on its use by police services in certain \ncircumstances might not necessarily apply to all stakeholders. He confirmed that the \nrecommendations made by the commissioners can apply to the use of FRT in public \nspaces as well. \nLegislation \nWith respect to regulating FRT and AI, most witnesses agreed that, while the current \nlegislative framework provides some protections, it is insufficient. \nFor example, Mr. Therrien said that there is a patchwork of laws that govern FR: the \nCanadian Charter of Rights and Freedoms, the common law and certain other laws, \nincluding privacy legislation.73 The problem, he says, is that this patchwork of laws can \nbe used in many ways. He believes that current rules are too vague to give the necessary \nlevel of trust that citizens should have in the collection of information by the public and \nprivate sector. \nMs. Piovesan made similar comments. She said there are some protections under \ncurrent federal privacy laws (the Privacy Act and PIPEDA) that apply to the use of FRT. \nFor example, she said that the PIPEDA requires companies to obtain consent to collect \nhighly sensitive data and, for public actors, regulation and the common law govern how \ncertain information can be collected, stored and retained. However, she said that there \nis no comprehensive or really focused law around FRT. \nProf. Thomasen explained that facial surveillance systems are socio-technical systems. \nThey cannot be understood only by looking at how a system is built: “One must also look \nat how it will interact with the people who use it, the people affected by it and the social \nenvironments in which it is deployed.” She added that part of the socio-technical context \nin which facial surveillance is introduced includes gaps in the application and underlying \ntheories of laws of general application, and these laws do not adequately protect against \nmisuses of this technology. \n \n73 \nCMTD and CPE Brief, p. 4; ICLMG Brief, p. 4. These stakeholders also referred to Canada’s obligations under \narticle 12 of the Universal Declaration of Human Rights and articles 17 and 21 of the International Covenant \non Civil and Political Rights. \n \n52 \nThe Committee notes that Bill C-27, An Act to enact the Consumer Privacy Protection \nAct, the Personal Information and Data Protection Tribunal Act and the Artificial \nIntelligence and Data Act and to make consequential and related amendments to other \nActs, introduced in the House of Commons in June 2022, if passed in its current form, \ncould address certain gaps in the current privacy legislative framework that may apply to \nFRT and AI.74 However, since the bill has not yet been passed, the Committee is making \nits recommendations based on the legislative framework in place. \nLegislative Framework for the Public and Private Sector \nTo shape a relatively comprehensive regulatory framework for FRT that mitigates the \nthreat this technology poses and takes advantage of its real beneficial possibilities, \nMs. Piovesan said that Canada should consider four AI principles that align with the \nOrganisation for Economic Co-operation and Development (OECD) AI Principles75 and \nleading international guidance on responsible AI. The OECD AI Principles are technical \nrobustness, accountability, lawfulness and fairness. \nFor example, she said that, with respect to technical robustness, questions that should \ninform regulation include what specific technical criteria ought to be associated with FRT \nuse cases and whether independent third parties should be engaged as oversight to \nassess FRT from a technical perspective. In terms of accountability, questions include \nwhat administrative controls should be required (e.g., an impact assessment), how are \nthose controls determined and by whom, and what stakeholders should be consulted. \nWith respect to lawfulness, questions include what oversight is needed to promote \nalignment of FRT uses with societal values and the law. Finally, with respect to fairness, \nquestions about the adverse effects of FRT on rights and freedoms and ways to minimize \nthese effects must be asked.76 \nThe Committee notes that witnesses have proposed various legislative measures that \nwould address many of the issues raised by Ms. Piovesan. \n \n74 \nLegisInfo, Bill C-27, An Act to enact the Consumer Privacy Protection Act, the Personal Information and Data \nProtection Tribunal Act and the Artificial Intelligence and Data Act and to make consequential and related \namendments to other Acts, 44th Parliament, 1st Session (Bill C-27). The bill was introduced in the House of \nCommons on 16 June 2022; Sabrina Charland, Alexandra Savoie, Ryan van den Berg, Legislative Summary of \nBill C-27: An Act to enact the Consumer Privacy Protection Act, the Personal Information and Data Protection \nTribunal Act and the Artificial Intelligence and Data Act and to make consequential and related amendments \nto other Acts, Publication No. 44-1-C27-E, 12 July 2022. \n75 \nOrganisation for Economic Co-operation and Development (OECD), OECD AI Principles overview. \n76 \nETHI, Evidence, Carole Piovesan. \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n53 \nWith respect to enforcement, several witnesses recommended granting the federal \nprivacy commissioner greater powers, including the power to make orders and impose \nstiff fines such as those found in the General Data Protection Regulation (GDPR) from \nthe European Union.77 Mr. Therrien himself said that the powers of his office should be \nstrengthened to make its decisions binding.78 \nAs for consent, Ms. Piovesan said that, although it depends on the use case, consent \nshould not be thrown out as a requirement for immutable biometric data. She said that \n“[h]aving appropriate notice, with some ability for that individual to make decisions \nabout how they share that information or how it’s collected, is really critical.” Ms. Khoo \nalso agreed that Canadians must be able to give prior and informed consent to their data \nbeing collected. \nDr. LaPlante said that regulations need to provide FRT developers, deployers and users \nwith clear requirements and obligations regarding the specific uses of this technology, \nincluding “the requirement to gain affirmed consent for the collection and use of \nbiometric data, as well as purpose limitation to avoid function creep.” She added, \nhowever, that regulations should seek to take a balanced approach that reduces the \nadministrative and financial burdens for public and private entities where possible. \nMr. Israel recommended that both the Privacy Act and PIPEDA be amended so that the \ncollection, use and disclosure of biometric information requires express consent in all \ncontexts. He also recommended that biometric information be defined as sensitive, as is \nthe case in Quebec law.79 Ms. Bhandari said that a consent requirement before \nbiometrics are captured is critical. Mr. Labonté said people need to be aware of how \ntheir data is going to be used and give informed consent. \nHowever, Ms. Poitras said that obtaining consent from people in the context of FR is not \nalways appropriate because there is a power asymmetry, whether between the citizen \nand the state or the citizen and a major corporation, like the web giants.80 She explained \n \n77 \nETHI, Evidence, Cynthia Khoo; ETHI, Evidence, Carole Piovesan; ETHI, Evidence, Tim McSorley; ETHI, \nEvidence, Brenda McPhail; ETHI, Evidence, Tamir Israel; ICLMG Brief, pp. 8–10. \n78 \nBill C-27, if passed in its current form, would grant the Privacy Commissioner the power to make binding \norders. However, it would not grant the Commissioner the power to impose fines or penalties. This power \nwould be granted to the new tribunal established by the bill. See sections 94 and 95 of the proposed \nConsumer Privacy Protection Act (CPPA). \n79 \nSee also: CMTD and CPE Brief, p. 5. These stakeholders also recommended that federal privacy laws be \namended to provide special protection for biometric information, including notice and consent prior to its \nuse or legislative permission. \n80 \nSee also: ETHI, Evidence, Elizabeth Anne Watkins. \n \n54 \nthat the way to mitigate consent is to legally authorize acceptable uses and prohibit \nothers uses, because even with consent or authorization, they are not appropriate in a \ndemocratic society. \nMr. McSorley recommended that private sector privacy laws be based on human rights \nand on necessity and proportionality. Regulations should have clear rules on consent, \nand the rules should apply to AI oversight and development in the private sector.81 \nDr. LaPlante also said that FRT legislation should be based on the principles of necessity \nand proportionality. \nWith respect to the public sector, Mr. McSorley recommended the clear establishment \nof no-go zones and clear rules around the issuance of privacy impact assessments. He \nalso recommended having a mandatory review of algorithmic and biometric surveillance \ntools used by law enforcement to assess their human rights impact, accuracy, and bias. \nMs. Polsky said that laws must be enacted requiring everyone who creates, purchases or \nuses technology to demonstrate a clear and correct grasp of Canadian laws and privacy \nrights. In the same way that vehicles and foods must meet stringent government \nregulations before being allowed for sale or use, creators of technologies should be \nsubject to laws requiring that technologies undergo comprehensive independent \nexamination of their privacy access and algorithmic integrity as well as their bias and \nimpact.82 \nMs. Polsky suggested that the technology be tested in a neutral sandbox run by the \nPrivacy Commissioner and involving other civil society groups to approve it before it is \nallowed for sale in Canada. The Centre for Media, Technology and Democracy and the \nCybersecure Policy Exchange recommended that the Privacy Act and PIPEDA be \nharmonized with the federal government’s Directive on Automated Decision-Making.83 \nMr. Israel suggested a similar approach. He argued that the onus is on the government \nto justify the use of FRT. He recommended that the Privacy Act and PIPEDA be amended \nto legally require companies and government agencies to file impact assessments with \n \n81 \nIbid. \n82 \nIbid. Section 15 of the Artificial Intelligence and Data Act created by Bill C-27 authorizes the Minister \ndesignated under the Act to order, if the Minister has reasonable grounds to believe that a person has \ncontravened the requirements under the Act, that an organization conduct an audit of the possible \ncontravention or engage an independent auditor to conduct the audit and provide a report to the Minister. \nThe audit would not be done before the product is commercialized. The obligations in the Artificial \nIntelligence and Data Act would not apply to government institutions. \n83 \nCMTD and CPE Brief, p. 5. \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n55 \nthe Privacy Commissioner prior to adopting intrusive technologies. The Commissioner \nshould be empowered to review these technologies through a public regulatory process \nand to put in place usage limitations or even moratoria where necessary.84 \nMr. Israel also said it is important to legislate a human in the decision-making loop on AI \ntechnologies, although he noted that human intervention does not solve all bias \nproblems. He said that, when using facial recognition systems, the tendency is to trust in \nthe automated results and assume an accurate match, which can end up embedding \ncognitive biases. Mr. Farooq also agreed that human checks are important, but said that \nin law enforcement, for example, given the problem of systemic racism and bias within \npolice agencies, courts are the place to get those checks and balances. \nWith this understanding, Mr. Mohammad recommended that the government put \nforth clear privacy legislation that severely curtails how FRT can be used in the non-\nconsumer context. According to the NCCM, such a law should impose a blanket ban on \nFRT by the government without judicial authorization, particularly for national security \nagencies. It should also set out clear penalties for agencies that violate privacy rules. \nMr. Farooq said that the process for using FRT should be similar to the process police \nmust follow to obtain a search warrant: appear before a judge and put forward an \nargument with clear documentation (evidence).85 \nWith respect to the PIPEDA, Mr. Therrien explained that principles-based, technology-\nneutral legislation for the private sector makes sense as a starting point. However, he \nsaid that FRT shows the limits of the virtues of a principles-based approach, because \nsuch an approach leaves a lot of discretion, for example to the police, to exercise those \nbroad principles in a way that suits their interests. Because of the risks of FRT, \nMr. Therrien said there ought to be specific provisions to prohibit uses except in certain \n \n84 \nBill C-27 proposes to create an Artificial Intelligence and Data Act (AI Act), which imposes certain \nrequirements on high impact artificial intelligence systems. The Privacy Commissioner is not responsible for \nthe administration of this Act. That responsibility will rest with the Minister of Industry, or the minister \ndesignated under the Act. The minister may designate a senior official of the department over which the \nMinister presides to be called the Artificial Intelligence and Data Commissioner. There is no explicit \nrequirement for privacy impact assessments in the proposed AI Act. \n85 \nETHI, Evidence, Mustafa Farooq. \n \n56 \ncircumstances.86 He said that the commissioners recommend that legislation define \n“allowable” and “prohibited” uses. \nFor her part, Ms. McPhail told the Committee that attention must be paid not only to \ntechnical privacy protections, but also to contextually relevant protections for the full set \nof rights engaged by this technology. She recommended a cross-sector data protection \nlaw grounded in a human rights framework. She noted that targeted laws governing \nbiometrics or algorithmically driven technologies could be even better fit for purpose. A \ncomprehensive and effective legislation that applies to FRT should provide a clear legal \nframework for the use of FRT, rigorous accountability and transparency provisions, \nindependent oversight and effective means of enforcement for failure to comply. \nMr. Therrien also noted that FRT brings rights other than to privacy into play, such as \nthe right to equality and democratic rights. He said it was therefore possible to have a \nnumber of regulatory agencies, including the OPC, responsible for oversight. For \nexample, the Canadian Human Rights Commission or its provincial equivalents could be \nresponsible in cases of discrimination. \nMr. Khanna said that legislation on FRT should be based on research and insight on \nracialized minorities, First Nations, children and anyone who is more vulnerable to this \nsort of exploitation. For example, he recommended consulting UNICEF’s policy guidance \non AI for children.87 He also underscored the need to draw on what people know within \nindustry to equalize and create a proper symmetry between what legislators know and \nwhat companies using that technology know. \nMs. Polsky, on the other hand, said that standards should be set without the direct or \nindirect influence or input of industry.88 She also suggested replacing fragmented \nlegislation at the federal, provincial and territorial levels with one overarching piece of \nlegislation that covers the public sector, the private sector, the non-profit sector and \npolitical parties. \n \n86 \nBill C-27, if passed in its current form, creates an Artificial Intelligence and Data Act that regulates \ninternational and interprovincial trade and commerce in artificial intelligence systems by establishing \ncommon requirements, applicable across Canada, for the design, development and use of those systems. \nThe Act would also prohibit certain conduct in relation to artificial intelligence systems that may result in \nserious harm to individuals or harm to their interests. \n87 \nUNICEF, Policy Guidance on AI for Children. The report highlights toys that interact with children through AI \nand the risks they pose around children’s security and privacy (p. 24). \n88 \nETHI, Evidence, Sharon Polsky. \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n57 \nFinally, some witnesses suggested that changes could also be made to non-privacy \nlegislation. \nFor example, Mr. Therrien said that, if certain circumstances required a court warrant for \nthe use of FRT, amendments to the Criminal Code may be required. Mr. Israel \nrecommended amending the Criminal Code to limit law enforcement use of FRT to \ninvestigations of serious crimes and in the absence of reasonable grounds to believe. \nMr. Farooq mentioned the possibility of amendments to the Royal Canadian Mounted \nPolice Act or the Canadian Security Intelligence Service Act, but did not specify which \nprovisions should be amended. \nLegislative Framework for Police Services \nMs. Kosseim said the commissioners’ main recommendation is that police agencies \nestablish a comprehensive statutory regime governing their use of FRT. Clear guardrails \nwith force of law are necessary to ensure that police agencies can make use of FRT, \ngrounded in a transparent framework and capable of earning the public’s enduring trust. \nMr. Therrien explained that he and his colleagues believe that the legislative framework \nthat should apply to the use of FRT by police agencies should be based on four \nelements: \nFirst, we recommend that the law clearly and explicitly define the purposes for which \npolice would be authorized to use facial recognition technology and that it prohibit \nother uses. Authorized purposes should be compelling and proportionate to the very \nhigh risks of the technology. \nSecond, since it is not realistic for the law to anticipate all circumstances, it is important, \nin addition to limitations on authorized purposes, that the law also require police use of \nfacial recognition to be both necessary and proportionate for any given deployment of \nthe technology. \nThird, we recommend that police use of facial recognition should be subject to strong, \nindependent oversight. Oversight should include proactive engagement measures such \nas privacy impact assessments, or PIAs; program level authorization or advance \nnotification before use; and powers to audit and make orders. \nFinally, we recommend that appropriate privacy protections be put in place to mitigate \nrisks to individuals, including measures addressing accuracy, retention and transparency \nin facial recognition initiatives. \nMr. Therrien raised the possibility, for example, that a police force’s program to use FRT \nrequire a privacy commissioner’s authorization. He suggested that, once the technology \n \n58 \nis adopted and actually used, oversight should include the authority to investigate \ncomplaints and make orders as to the lawfulness of the use of the technology in a \ngiven case. \nThe Canadian Human Rights Commission indicated that the legal framework for police \nuse of FRT should take a human-rights-based approach that integrates protections for \nchildren and youth. Such an approach uses international human rights as a foundation.89 \nBest Practices in Other Jurisdictions \nMs. Poitras said that, in Quebec, biometric databases and the use of biometrics for \nidentification purposes are governed by the Act to establish a legal framework for \ninformation technology and by privacy statutes applicable to public and private \norganizations.90 Under this act, the creation of every biometric database must be \nreported to the CAI. As of September 2022, reporting will also be required for \nevery instance in which biometrics are used for identification purposes. Ms. Poitras said \nthat Quebec’s act could be improved by expanding its scope. The act establishes \nobligations only where biometrics, including FRT, are used to verify identity. However, \nFRT can be used for other purposes. \nMs. Poitras explained that, in Quebec: \n[B]iometrics may not be used for identification purposes without the express consent of \nthe person concerned. No biometric characteristic may be recorded without that \nperson’s knowledge. Only a minimum number of biometric characteristics may be \nrecorded and used. Any other information that may be discovered based on those \ncharacteristics may not be used or preserved. Lastly, biometric information and any \nnote concerning that information must be destroyed when the purpose of the \nverification or confirmation of identity has been achieved. The commission has broad \nauthority and may make any order respecting biometric banks, including authority to \nsuspend or prohibit their bringing into service or order their destruction. General \nprivacy protection rules also apply in addition to these specific provisions. That means, \nfor example, that the use of facial recognition must be necessary and proportionate to \nthe objective pursued. \n \n89 \nCHRC Brief, pp. 5–8. The five elements of a human-rights-based approach are legality, non-discrimination, \nparticipation, empowerment and accountability. \n90 \nQuebec, Act to establish a legal framework for information technology, chapter C-1.1; Quebec, Act \nrespecting access to documents held by public bodies and the protection of personal information [Quebec \npublic sector act] chapter A-2.1; Quebec, Act respecting the protection of personal information in the private \nsector, chapter P-39.1 [Quebec private sector act]. \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n59 \nMs. Poitras added that a privacy impact assessment will be mandatory in Quebec as of \nSeptember 2023 and that biometric information will be expressly designated as sensitive \npersonal information.91 \nIn the U.S., Mr. Larter said that Washington State passed legislation in 2021 that lays out \nimportant transparency and accountability measures for the use of FRT, including a \ntesting requirement and oversight by an appropriately trained human.92 In Utah, the bill \nentitled S.B. 34 Governmental Use of Facial Recognition Technology, passed in 2021, \nrequires government entities to notify individuals whenever they are capturing images \nthat could be used in conjunction with FRT and to provide 30 days prior notice to the \nproposed use.93 \nIn 2008, Illinois passed the Biometric Information Privacy Act (BIPA). The act prohibits \ncompanies from selling or otherwise profiting from consumers’ biometric information.94 \nDr. Watkins said that the BIPA allowed a lawsuit to be filed against Facebook for using \nfacial recognition in their photo identification processes.95 Ms. Bhandari added that the \nact also allowed the ACLU to file a lawsuit against Clearview AI, which resulted in a \nsettlement. Under the settlement, the company can no longer provide access to its \ndatabase containing hundreds of millions of face prints to private entities across the \nU.S., with a few exceptions, and is banned from selling its technology to Illinois law \nenforcement for five years.96 \nMs. Bhandari advocated for the adoption of legislation like the Illinois BIPA, but with \nsome updates. She said biometric privacy law should clearly require companies to obtain \nnotice and written consent before collecting, using, or disclosing any person’s identifier. \nIt should prohibit companies from withholding services from people who choose not to \n \n91 \nNational Assembly of Quebec, Bill 64, An Act to modernize legislative provisions as regards the protection of \npersonal information. Passed in September 2021, this bill amends the Quebec public sector act and the \nQuebec private sector act to include this obligation. With some exceptions, the provisions of the bill come \ninto force on 22 September 2023. \n92 \nWashington State Legislature, SB 6289 – 2019-20, Concerning the use of facial recognition services. Under \nthe bill, Washington State and local agencies, including law enforcement, that use or plan to use facial \nrecognition technology must meet certain reporting and deployment requirements. \n93 \nOPC, Letter to the Committee, 13 May 2022, p. 3. \n94 \nIbid. \n95 \nThe Committee invited Facebook to appear before it as part of its study, but it declined, stating that \nFacebook no longer uses facial recognition technology; Meta, An Update On Our Use of Face Recognition, \n2 November 2021. Google was also invited but declined. Amazon accepted an invitation to testify but could \nnot appear after a change in schedule. None of these companies submitted a brief. \n96 \nACLU, In Big Win, Settlement Ensures Clearview AI Complies With Groundbreaking Illinois Biometric Privacy \nLaw, 9 May 2022, News release. \n \n60 \nconsent. It should also require businesses to delete biometric identifiers after one year \nof the individual’s last interaction with the business.97 \nMs. Bhandari named two other models to follow. In Maine, the proposed An Act To \nRegulate the Use of Biometric Identifiers would require private entities to obtain the \nexpress consent of individuals to collect, use and disclose biometric identifiers. The law \nwould also prohibit the sale of biometric data and impose limits on data storage. In \nMaryland, a bill entitled the Biometric Data Privacy Act has been introduced. It seeks to \ncreate a framework similar to the one in Illinois.98 \nMr. Therrien mentioned federal laws proposed in the U.S., including the Fourth \nAmendment Is Not For Sale Act, which would stop data brokers from selling personal \ninformation to law enforcement agencies without court oversight and would ban the use \nof data that was illegally obtained by public agencies.99 The Algorithmic Accountability \nAct would require private organizations to conduct assessments of automated decision \nsystems for algorithmic bias and effectiveness.100 \nWith respect to Europe, several witnesses said that the GDPR is a model for data \nprotection.101 Under the GDPR, biometrics, including facial images, are considered a \nspecial category of data that is prohibited, unless the controller can rely upon a legal \nground and a ground for processing.102 \nMs. Piovesan said that the GDPR includes a right to recourse and a right to objection on \nprofiling solely by automatic means.103 She also said that, under the GDPR, fines can be \nimposed for the use of data of European residents, even if the actual activity does not \n \n97 \nBill C-27, if passed in its current form, contains a right to opt out in section 55 of the new CPPA. \n98 \nACLU, LD 1945 Biometric Identifiers: Fact Sheet, Document submitted to the ETHI Committee, 21 June 2022 \n(Maine bill); ACLU, HB 259 – The Biometric Data Privacy Act: Amendment Recommendations & Fact Sheet \n(Maryland), Document submitted to the ETHI Committee, 21 June 2022 (Maryland bill). The ACLU said that \nseveral amendments made to the text of the original bill by the Maryland House of Representatives \nweakened the bill. \n99 \nOPC, Letter to the Committee, 13 May 2022, p. 2. An identical bill was introduced in the Senate: S.1265 - \nFourth Amendment Is Not For Sale Act. \n100 \nOPC, Letter to the Committee, 13 May 2022, p. 2. An identical bill was introduced in the Senate: S.3572 - \nAlgorithmic Accountability Act of 2022. \n101 \nETHI, Evidence, Ana Brandusescu; ETHI, Evidence, Elizabeth Anne Watkins. \n102 \nEuropean Union, EUR-Lex, Regulation (EU) 2016/679 of the European Parliament and of the Council of \n27 April 2016 on the protection of natural persons with regard to the processing of personal data and on \nthe free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) \n(Text with EEA relevance); OPC, Letter to the Committee, 13 May 2022, p. 2. \n103 \nShe noted that a similar right exists under Quebec law as well. \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n61 \ntake place in the European jurisdiction. The GDPR therefore has extra-jurisdictional \napplicability. Dr. Watkins said that the GDPR contains a right to an explanation that \nensures, for example, that companies have to provide workers with insights into how \ndecisions are made about them by automated systems.104 \nThe European Union directive on the protection of individuals with regard to the \nprocessing of personal data by competent authorities for the purposes of the \nprevention, investigation, detection or prosecution of criminal offences or the execution \nof criminal penalties, and on the free movement of such data, was also raised by \nwitnesses.105 It forbids law enforcement from processing biometric data for the purpose \nof uniquely identifying a person except where authorized by law and from making \ndecisions based solely on automated processing, including profiling, unless European \nUnion or domestic law provides appropriate safeguards for individual rights and \nfreedoms.106 \nFinally, some witnesses brought up the European Union’s proposed Artificial Intelligence \nAct.107 Mr. Therrien explained that the act, if adopted: \nwould outlaw public and private sectors from using harmful AI applications that, among \nother things, manipulate individuals or exploit vulnerabilities of individuals due to \ncertain personal characteristics. Non-prohibited applications that are high-risk (including \nuse of biometrics for identification and categorization) are subject to specific legal \nrequirements such as risk management measures and systems; logging and record-\nkeeping; general human oversight; accurate and representative data for AI training; ex-\nante conformity assessments; and, demonstrable accountability.108 \nMr. Therrien said that the European proposal protects constitutional and human rights. \nMs. Kosseim and Ms. Poitras agreed with him. \n \n104 \nBill C-27, if passed in its current form, contains a right to an explanation in sections 63 and 64 of the CPPA. \n105 \n“Directive (EU) 2016/680 of the European Parliament and of the Council of 27 April 2016 on the protection \nof natural persons with regard to the processing of personal data by competent authorities for the purposes \nof the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal \npenalties, and on the free movement of such data, and repealing Council Framework Decision \n2008/977/JHA,” Official Journal of the European Union. \n106 \nCMTD and CPE Brief, p. 8. \n107 \nEuropean Commission, Proposal for a Regulation of the European Parliament and of the Council Laying \nDown Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) and Amending Certain Union \nLegislative Acts; ETHI, Evidence, Petra Molnar; ETHI, Evidence, Cynthia Khoo; ETHI, Evidence, Alex LaPlante. \n108 \nOPC, Letter to the Committee, 13 May 2022, p. 2. \n \n62 \nMs. Piovesan said that a risk-based approach to the regulation of AI, as the European \nproposal does, is seen in other jurisdictions. The European proposal would also prohibit \nthe use of real-time FRT in public spaces for law enforcement purposes.109 \nDr. Molnar said the EU proposal recognizes that individual risk assessments for the \npurposes of immigration and refugee processing are high-risk and bans assessments that \ncan be used for profiling and for strengthening systemic discrimination. \nWith respect to the United Kingdom, Prof. Jenkins brought up the Surveillance Camera \nCode of Practice, which provides guidance on the appropriate use of surveillance camera \nsystems by local authorities or police. The code states that the use of FRT should always \ninvolve human intervention before decisions are taken that affect an individual \nadversely.110 The Biometrics and Surveillance Camera Commissioner is an independent \nmonitoring body that encourages compliance with the code of practice.111 Scotland has \nalso had a biometrics commissioner since 2020, who published a draft code of practice \nin April 2022.112 \nCommittee Observations and Recommendations \nThe Committee found that witnesses clearly demonstrated the inadequacy of the \ncurrent legislative framework for the regulation of FRT and AI. The Committee therefore \nmakes the following recommendations: \nRecommendation 11 \nThat the government define in appropriate legislation acceptable uses of facial \nrecognition technology or other algorithmic technologies and prohibit other uses, \nincluding mass surveillance. \n \n109 \nETHI, Written response submitted to the Committee by Sharon Polsky, 17 June 2022; European \nParliamentary Research Service, STOA study on diverging obligations facing public and private sector \napplications of artificial intelligence. \n110 \nUnited Kingdom, Home Office, Surveillance Camera Code of Practice, 2021. \n111 \nUnited Kingdom, Biometrics and Surveillance Camera Commissioner; See also: ETHI, Written response \nsubmitted to the Committee by Sharon Polsky, 17 June 2022. \n112 \nScotland, Scottish Biometrics Commissioner; United Kingdom, Scottish Biometrics Commissioner Act 2020; \nSee also: ETHI, Written response submitted to the Committee by Sharon Polsky, 17 June 2022. In April 2022, \nthe Scottish Commissioner published a draft code of practice on the acquisition, retention, use and \ndestruction of biometric data for criminal justice and police purposes in Scotland. \nFACIAL RECOGNITION TECHNOLOGY AND  \nTHE GROWING POWER OF ARTIFICIAL INTELLIGENCE \n63 \nRecommendation 12 \nThat the Government of Canada amend the Privacy Act to require that prior to the \nadoption, creation, or use of facial recognition technology, government agencies seek \nthe advice and recommendations of the Privacy Commissioner, and file impact \nassessments with his or her office. \nRecommendation 13 \nThat the Government of Canada update the Canadian Human Rights Act to ensure that it \napplies to discrimination caused by the use of facial recognition technology and other \nartificial intelligence technologies. \nRecommendation 14 \nThat the Government of Canada implement the right to erasure (“right to be forgotten”) \nby requiring service providers, social media platforms and other online entities operating \nin Canada to delete all users’ personal information after a set period following users’ \ntermination of use, including but not limited to uploaded photographs, payment \ninformation, address and contact information, posts and survey entries. \nRecommendation 15 \nThat the Government of Canada implement an opt-in-only requirement for the collection \nof biometric information by private sector entities and prohibit such entities from \nmaking the provision of goods or services contingent on providing biometric information. \nRecommendation 16 \nThat the Government of Canada strengthen the ability of the Privacy Commissioner to \nlevy meaningful penalties on government institutions and private entities whose use of \nfacial recognition technology violates the Privacy Act or the Personal Information \nProtection and Electronic Documents Act to deter future abuse of the technology. \nRecommendation 17 \nThat the Government of Canada amend the Privacy Act and the Personal Information \nProtection and Electronic Documents Act to prohibit the practice of capturing images of \nCanadians from the internet or public spaces for the purpose of populating facial \nrecognition technology databases or artificial intelligence algorithms. \n \n64 \nRecommendation 18 \nThat the Government of Canada impose a federal moratorium on the use of facial \nrecognition technology by (Federal) policing services and Canadian industries unless \nimplemented in confirmed consultation with the Office of the Privacy Commissioner or \nthrough judicial authorization; that the Government actively develop a regulatory \nframework concerning uses, prohibitions, oversight and privacy of facial recognition \ntechnology; and that the oversight should include proactive engagement measures, \nprogram level authorization or advance notification before use, and powers to audit and \nmake orders. \nRecommendation 19 \nThat the federal government ensure that appropriate privacy protections are put in place \nto mitigate risks to individuals, including measures addressing accuracy, retention and \ntransparency in facial recognition initiatives as well as a comprehensive strategy around \ninformed consent by Canadians for the use of their private information. \nCONCLUSION \nThe Committee’s study confirmed that Canada’s current legislative framework does not \nadequately regulate FRT and AI. Without an appropriate framework, FRT and other AI \ntools could cause irreparable harm to some individuals. \nThe Committee is therefore of the view that, when FRT or other AI technology is used, \nthey must be used responsibly, within a robust legislative framework that protects \nCanadians’ privacy rights and civil liberties. Since such a legislative framework does not \nexist at the time, a national pause should be imposed on the use of FRT, particularly with \nrespect to police services. \nThe Committee strongly encourages the Government of Canada to implement its \nrecommendations as quickly as possible. \n65 \nAPPENDIX A \nLIST OF WITNESSES \nThe following table lists the witnesses who appeared before the committee at its \nmeetings related to this report. Transcripts of all public meetings related to this report \nare available on the committee’s webpage for this study. \nOrganizations and Individuals \nDate \nMeeting \nAs an individual \nAna Brandusescu, Artificial Intelligence Governance Expert \nCynthia Khoo, Research Fellow \nThe Citizen Lab, Munk School of Global Affairs and Public \nPolicy, University of Toronto \nKristen Thomasen, Professor \nPeter A. Allard, School of Law, University of British \nColumbia \n2022/03/21 \n11 \nINQ Law \nCarole Piovesan, Managing Partner \n2022/03/21 \n11 \nRefugee Law Lab \nPetra Molnar, Lawyer \nYork University \n2022/03/21 \n11 \nBorealis AI \nAlex LaPlante, Senior Director \nProduct and Business Engagement \n2022/03/24 \n12 \nCanadian Civil Liberties Association \nBrenda McPhail, Director \nPrivacy, Technology and Surveillance Program \n2022/03/24 \n12 \nComputer Research Institute of Montréal \nFrançoys Labonté, Chief Executive Officer \n2022/03/24 \n12 \nInternational Civil Liberties Monitoring Group \nTim McSorley, National Coordinator \n2022/03/24 \n12 \n66 \nOrganizations and Individuals \nDate \nMeeting \nAs an individual \nRob Jenkins, Professor \nUniversity of York \nSanjay Khanna, Strategic Advisor and Foresight Expert \nAngelina Wang, Computer Science Graduate Researcher \nPrinceton University \nElizabeth Anne Watkins, Postdoctoral Research Associate \nPrinceton University \n2022/04/04 \n15 \nRoyal Canadian Mounted Police \nAndré Boileau, Officer in Charge \nNational Child Exploitation Crime Centre \nPaul Boudreau, Acting Deputy Commissioner \nSpecialized Policing Services \n2022/04/28 \n17 \nToronto Police Service \nColin Stairs, Chief Information Officer \n2022/04/28 \n17 \nToronto Police Services Board \nDubi Kanengisser, Senior Advisor \nStrategic Analysis and Governance \n2022/04/28 \n17 \nCommission d'accès à l'information du Québec \nDiane Poitras, President \n2022/05/02 \n18 \nOffice of the Information and Privacy Commissioner \nof Ontario \nPatricia Kosseim, Commissioner \nVance Lockton, Senior Technology and Policy Advisor \n2022/05/02 \n18 \nOffice of the Privacy Commissioner of Canada \nDaniel Therrien, Privacy Commissioner of Canada \nDavid Weinkauf, Senior Information Technology Research \nAnalyst \n2022/05/02 \n18 \nMicrosoft \nOwen Larter, Director \nResponsible Artificial Intelligence Public Policy \n2022/05/05 \n19 \nNational Council of Canadian Muslims \nMustafa Farooq, Chief Executive Officer \nRizwan Mohammad, Advocacy Officer \n2022/05/05 \n19 \n67 \nOrganizations and Individuals \nDate \nMeeting \nRoyal Canadian Mounted Police \nAndré Boileau, Officer in Charge \nNational Child Exploitation Crime Centre \nGordon Sage, Director General \nSensitive and Specialized Investigative Services \nRoch Séguin, Director \nStrategic Services Branch, Technical Operations \n2022/05/09 \n20 \nToronto Police Service \nColin Stairs, Chief Information Officer \n2022/05/09 \n20 \nAs an individual \nNestor Maslej, Research Associate \nInstitute for Human-Centered Artificial Intelligence, \nStanford University \n2022/06/09 \n25 \nPrivacy and Access Council of Canada \nSharon Polsky, President \n2022/06/09 \n25 \nAmerican Civil Liberties Union \nEsha Bhandari, Deputy Director \n2022/06/16 \n27 \nSamuelson-Glushko Canadian Internet Policy and \nPublic Interest Clinic \nTamir Israel, Staff Lawyer \n2022/06/16 \n27 \n \n \n \n \n69 \nAPPENDIX B \nLIST OF BRIEFS \nThe following is an alphabetical list of organizations and individuals who submitted briefs \nto the committee related to this report. For more information, please consult the \ncommittee’s webpage for this study. \nCanadian Human Rights Commission  \nCentre for Media, Technology and Democracy  \nCybersecure Policy Exchange \nJenkins, Rob  \nInternational Civil Liberties Monitoring Group \nLigue des droits et libertés \nMaslej, Nestor \nRefugee Law Lab \nTessono, Christelle \n \n \n \n71 \nREQUEST FOR GOVERNMENT RESPONSE \nPursuant to Standing Order 109, the committee requests that the government table a \ncomprehensive response to this Report. \nA copy of the relevant Minutes of Proceedings (Meetings Nos. 11, 12, 15, 17-20, 25, 27, \n28, 34 and 35) is tabled. \nRespectfully submitted, \nPat Kelly \nChair",
            "metadata": {
                "source_domain": "www.ourcommons.ca",
                "scrape_date": "2024-10-25T12:40:33.245364",
                "content_type": "pdf",
                "extraction_method": "pymupdf"
            }
        }
    ]
}